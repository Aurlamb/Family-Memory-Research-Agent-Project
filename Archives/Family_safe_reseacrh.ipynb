{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory project - Database vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load path from the environment variable\n",
    "env_ih1 = os.getenv(\"ENV_IH1\")\n",
    "\n",
    "dotenv_path = Path(env_ih1)\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY= os.getenv('PINECONE_KEY')\n",
    "SERPAPI_API_KEY = os.getenv('SERPAPI_API_KEY')\n",
    "STEAMSHIP_API_KEY = os.getenv('STEAMSHIP_API_KEY')\n",
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "GEMINI_KEY = os.getenv('GEMINI_KEY')\n",
    "\n",
    "os.environ['PATH'] += os.pathsep + '/usr/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Family safe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pinecone DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone as pc\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# connect to pinecone environment\n",
    "pc = Pinecone(\n",
    "    api_key = PINECONE_API_KEY,\n",
    "    environment='us-east-1'  # find next to API key in console\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"memory-project\"\n",
    "\n",
    "# check if the memory project index exists\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # create the index if it does not exist\n",
    "    pc.create_index(name=index_name, dimension=768, metric=\"cosine\", spec=spec)\n",
    "# connect to extractive-question-answering index we created\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test with pages as chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from pinecone import Pinecone\n",
    "import shutil\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer \n",
    "\n",
    "def process_json_files(data_dir='/Family safe', processed_dir=r'C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Family safe\\processed'):\n",
    "    \"\"\"\n",
    "    Fetches all JSON files from the specified directory, \n",
    "    creates a DataFrame from the JSON data, \n",
    "    processes the data, and moves processed files to the specified directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory containing the JSON files.\n",
    "        processed_dir: Path to the directory where processed files will be moved.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the combined data from all JSON files.\n",
    "    \"\"\"\n",
    "\n",
    "    all_data = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            with open(filepath, 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "            all_data.extend(json_data)\n",
    "\n",
    "            # Move processed file \n",
    "            processed_filepath = os.path.join(processed_dir, filename)\n",
    "            shutil.move(filepath, processed_filepath) \n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Data cleaning and transformation\n",
    "    df['Doc name'] = df['Name']  # Assuming 'Name' is in the JSON\n",
    "    df['Type'] = df['Type'].fillna('unknown') \n",
    "    df['Full text'] = df['Pages'].apply(lambda x: ' '.join([page['Text'] for page in x]) if x else \"\") \n",
    "\n",
    "    # Generate summaries using a language model \n",
    "    model_name = \"google/flan-t5-base\"  # Replace with your preferred summarization model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    df['Summary'] = df['Full text'].apply(lambda x: generate_summary(x, model, tokenizer))\n",
    "\n",
    "    df['Chunks'] = df['Pages'].apply(lambda x: [page['Text'] for page in x] if x else [])\n",
    "    df['processed_address'] = processed_filepath \n",
    "    df = df[['Doc name', 'Type', 'Author', 'Date', 'Summary', 'Chunks', 'Full text', 'processed_address']] \n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_summary(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a summary of the given text using a language model.\n",
    "\n",
    "    Args:\n",
    "        text: The input text.\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the language model.\n",
    "\n",
    "    Returns:\n",
    "        The generated summary.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(**inputs)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the given text using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        text: The text to generate embeddings for.\n",
    "\n",
    "    Returns:\n",
    "        A list of floats representing the embeddings.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "    embeddings = model.encode(text)\n",
    "    return embeddings\n",
    "\n",
    "def upload_to_pinecone(df, index_name, dimension):\n",
    "    \"\"\"\n",
    "    Uploads the DataFrame to a Pinecone index.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame containing the data.\n",
    "        index_name: The name of the Pinecone index.\n",
    "        dimension: The dimensionality of the embeddings.\n",
    "    \"\"\"\n",
    "    initialize(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENVIRONMENT\") \n",
    "    index = Pinecone.Index(index_name) \n",
    "\n",
    "    # Upsert both full text and summary to Pinecone\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row['processed_address']): \n",
    "            doc_id = row['Doc name'] \n",
    "            full_text_vector = get_embeddings(row['Full text'])\n",
    "            summary_vector = get_embeddings(row['Summary']) \n",
    "            index.upsert([\n",
    "                (doc_id + \"_full_text\", full_text_vector), \n",
    "                (doc_id + \"_summary\", summary_vector)\n",
    "            ]) \n",
    "\n",
    "# Get the final DataFrame\n",
    "final_df = process_json_files()\n",
    "\n",
    "# Upload the DataFrame to Pinecone\n",
    "index_name = \"your_index_name\" \n",
    "dimension = 768  # Dimensionality of your embeddings\n",
    "upload_to_pinecone(final_df, index_name, dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with chunks overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from pinecone import Pinecone, Index\n",
    "import shutil\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer \n",
    "\n",
    "def process_json_files(data_dir=r'.\\Family safe', processed_dir=r'.\\Family_safe\\processed', chunk_size=512, overlap=128):\n",
    "    \"\"\"\n",
    "    Fetches all JSON files from the specified directory, \n",
    "    creates a DataFrame from the JSON data, \n",
    "    processes the data, and moves processed files to the specified directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Path to the directory containing the JSON files.\n",
    "        processed_dir: Path to the directory where processed files will be moved.\n",
    "        chunk_size: Size of each chunk in tokens.\n",
    "        overlap: Size of the overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the combined data from all JSON files.\n",
    "    \"\"\"\n",
    "\n",
    "    all_data = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "\n",
    "            for doc in json_data:\n",
    "                if isinstance(doc, dict) and 'Type' in doc and doc['Type'] == 'scan':\n",
    "                    doc['Chunks'] = create_overlapping_chunks(doc['Pages'], chunk_size, overlap)\n",
    "                elif isinstance(doc, dict) and 'Pages' in doc:\n",
    "                    doc['Chunks'] = [[page['Text']] for page in doc['Pages']]\n",
    "                else:\n",
    "                    doc['Chunks'] = []\n",
    "\n",
    "            all_data.extend(json_data)\n",
    "\n",
    "            # Move processed file \n",
    "            processed_filepath = os.path.join(processed_dir, filename)\n",
    "            shutil.move(filepath, processed_filepath) \n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Data cleaning and transformation\n",
    "    df['Doc name'] = df['Name']  # Assuming 'Name' is in the JSON\n",
    "    df['Type'] = df['Type'].fillna('unknown') \n",
    "\n",
    "    # Extract full text from Chunks (if available)\n",
    "    df['Full text'] = df['Chunks'].apply(lambda x: ' '.join([chunk for sublist in x for chunk in sublist]) if x else \"\") \n",
    "\n",
    "    # Generate summaries using a language model \n",
    "    model_name = \"google/flan-t5-base\"  # Replace with your preferred summarization model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    df['Summary'] = df['Full text'].apply(lambda x: generate_summary(x, model, tokenizer))\n",
    "\n",
    "    df['processed_address'] = processed_filepath \n",
    "    df = df[['Doc name', 'Type', 'Author', 'Date', 'Summary', 'Chunks', 'Full text', 'processed_address']] \n",
    "\n",
    "    return df\n",
    "\n",
    "def create_overlapping_chunks(pages, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Creates overlapping chunks from a list of pages.\n",
    "\n",
    "    Args:\n",
    "        pages: List of pages, where each page is a dictionary with 'Text' and 'Page number'.\n",
    "        chunk_size: Size of each chunk in tokens.\n",
    "        overlap: Size of the overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        List of chunks, where each chunk is a list of page texts.\n",
    "    \"\"\"\n",
    "\n",
    "    all_text = ' '.join([page['Text'] for page in pages])\n",
    "    tokens = tokenizer.tokenize(all_text)\n",
    "    num_chunks = max(1, (len(tokens) - overlap) // (chunk_size - overlap)) \n",
    "    chunks = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start = i * (chunk_size - overlap)\n",
    "        end = start + chunk_size\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        # Determine which pages belong to this chunk\n",
    "        chunk_pages = []\n",
    "        current_chunk_len = 0\n",
    "        for page in pages:\n",
    "            page_tokens = tokenizer.tokenize(page['Text'])\n",
    "            if current_chunk_len + len(page_tokens) <= len(chunk_tokens):\n",
    "                chunk_pages.append(page['Text'])\n",
    "                current_chunk_len += len(page_tokens)\n",
    "        chunks.append(chunk_pages)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def generate_summary(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a summary of the given text using a language model.\n",
    "\n",
    "    Args:\n",
    "        text: The input text.\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the language model.\n",
    "\n",
    "    Returns:\n",
    "        The generated summary.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(**inputs)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the given text using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        text: The text to generate embeddings for.\n",
    "\n",
    "    Returns:\n",
    "        A list of floats representing the embeddings.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "    embeddings = model.encode(text)\n",
    "    return embeddings\n",
    "\n",
    "def upload_to_pinecone(df, index_name, dimension):\n",
    "    \"\"\"\n",
    "    Uploads the DataFrame to a Pinecone index.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame containing the data.\n",
    "        index_name: The name of the Pinecone index.\n",
    "        dimension: The dimensionality of the embeddings.\n",
    "    \"\"\"\n",
    "    # # itialize(PINECONE_API_KEY, environment=spec) \n",
    "    # index = Pinecone.Index(index_name) \n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row['processed_address']): \n",
    "            doc_id = row['Doc name'] \n",
    "            for i, chunk in enumerate(row['Chunks']):\n",
    "                chunk_id = f\"{doc_id}_{i}\" \n",
    "                chunk_text = ' '.join(chunk)\n",
    "                chunk_vector = get_embeddings(chunk_text)\n",
    "                index.upsert([(chunk_id, chunk_vector)]) \n",
    "\n",
    "# Get the final DataFrame\n",
    "final_df = process_json_files()\n",
    "\n",
    "# Upload the DataFrame to Pinecone\n",
    "dimension = 768  # Dimensionality of your embeddings\n",
    "upload_to_pinecone(final_df, index_name, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 165\u001b[0m\n\u001b[0;32m    162\u001b[0m                 index\u001b[38;5;241m.\u001b[39mupsert([(chunk_id, chunk_vector)])\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Get the final DataFrame\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_json_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Upload the DataFrame to Pinecone\u001b[39;00m\n\u001b[0;32m    168\u001b[0m dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m768\u001b[39m  \u001b[38;5;66;03m# Dimensionality of your embeddings\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 62\u001b[0m, in \u001b[0;36mprocess_json_files\u001b[1;34m(data_dir, processed_dir, chunk_size, overlap)\u001b[0m\n\u001b[0;32m     59\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_data)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Data cleaning and transformation\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDoc name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Extract document name\u001b[39;00m\n\u001b[0;32m     63\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Extract full text from Chunks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Name'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import shutil\n",
    "from pinecone import Index  # Import Index for Pinecone operations\n",
    "\n",
    "def process_json_files(data_dir=r'.\\Family safe', processed_dir=r'C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Family safe\\processed', chunk_size=512, overlap=128):\n",
    "    \n",
    "    \"\"\"\n",
    "    Processes all JSON files in the specified directory, extracts data, and generates chunks and summaries.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing JSON files.\n",
    "        processed_dir: Directory where processed files will be moved.\n",
    "        chunk_size: Size of each chunk in tokens.\n",
    "        overlap: Size of the overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the processed data.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "\n",
    "            # Handle JSON as a dictionary or list\n",
    "            if isinstance(json_data, dict):  # Single document\n",
    "                doc = json_data\n",
    "                if doc.get('Type') == 'scan' and 'Pages' in doc:\n",
    "                    doc['Chunks'] = create_overlapping_chunks(doc['Pages'], chunk_size, overlap)\n",
    "                elif 'Pages' in doc:\n",
    "                    doc['Chunks'] = [[page.get('Extracted Text', '')] for page in doc['Pages'] if isinstance(page, dict)]\n",
    "                else:\n",
    "                    doc['Chunks'] = []\n",
    "                all_data.append(doc)\n",
    "            elif isinstance(json_data, list):  # List of documents\n",
    "                for doc in json_data:\n",
    "                    if isinstance(doc, dict):\n",
    "                        if doc.get('Type') == 'scan' and 'Pages' in doc:\n",
    "                            doc['Chunks'] = create_overlapping_chunks(doc['Pages'], chunk_size, overlap)\n",
    "                        elif 'Pages' in doc:\n",
    "                            doc['Chunks'] = [[page.get('Extracted Text', '')] for page in doc['Pages'] if isinstance(page, dict)]\n",
    "                        else:\n",
    "                            doc['Chunks'] = []\n",
    "                        all_data.append(doc)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unexpected doc structure in list: {doc}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected JSON structure: {json_data}\")\n",
    "\n",
    "            # Move processed file\n",
    "            processed_filepath = os.path.join(processed_dir, filename)\n",
    "            shutil.move(filepath, processed_filepath)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Data cleaning and transformation\n",
    "    df['Doc name'] = df['Name']  # Extract document name\n",
    "    df['Type'] = df['Type'].fillna('unknown')\n",
    "\n",
    "    # Extract full text from Chunks\n",
    "    df['Full text'] = df['Chunks'].apply(\n",
    "        lambda x: ' '.join([chunk for sublist in x for chunk in sublist]) if x else \"\")\n",
    "\n",
    "    # Generate summaries using a language model\n",
    "    model_name = \"google/flan-t5-base\"  # Replace with your preferred summarization model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    df['Summary'] = df['Full text'].apply(lambda x: generate_summary(x, model, tokenizer))\n",
    "\n",
    "    df['processed_address'] = processed_filepath\n",
    "    df = df[['Doc name', 'Type', 'Author', 'Date', 'Summary', 'Chunks', 'Full text', 'processed_address']]\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_overlapping_chunks(pages, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Creates overlapping chunks from a list of pages.\n",
    "\n",
    "    Args:\n",
    "        pages: List of pages, where each page is a dictionary with 'Extracted Text'.\n",
    "        chunk_size: Size of each chunk in tokens.\n",
    "        overlap: Size of the overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        List of chunks, where each chunk is a list of page texts.\n",
    "    \"\"\"\n",
    "    all_text = ' '.join([page.get('Extracted Text', '') for page in pages if isinstance(page, dict)])\n",
    "    tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "    token_ids = tokenizer.encode(all_text)  # Encode the text into token IDs\n",
    "    num_chunks = max(1, (len(token_ids) - overlap) // (chunk_size - overlap))\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start = i * (chunk_size - overlap)\n",
    "        end = start + chunk_size\n",
    "        chunk_token_ids = token_ids[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_token_ids)  # Decode back to text\n",
    "        chunk_pages = []\n",
    "        current_chunk_len = 0\n",
    "        for page in pages:\n",
    "            page_tokens = tokenizer.encode(page.get('Extracted Text', ''))\n",
    "            if current_chunk_len + len(page_tokens) <= len(chunk_token_ids):\n",
    "                chunk_pages.append(page.get('Extracted Text', ''))\n",
    "                current_chunk_len += len(page_tokens)\n",
    "        chunks.append(chunk_pages)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def generate_summary(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a summary of the given text using a language model.\n",
    "\n",
    "    Args:\n",
    "        text: The input text.\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the language model.\n",
    "\n",
    "    Returns:\n",
    "        The generated summary.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(**inputs)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the given text using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        text: The text to generate embeddings for.\n",
    "\n",
    "    Returns:\n",
    "        A list of floats representing the embeddings.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(text)\n",
    "    return embeddings\n",
    "\n",
    "def upload_to_pinecone(df, index_name, dimension):\n",
    "    \"\"\"\n",
    "    Uploads the DataFrame to a Pinecone index.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame containing the data.\n",
    "        index_name: The name of the Pinecone index.\n",
    "        dimension: The dimensionality of the embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row['processed_address']):\n",
    "            doc_id = row['Doc name']\n",
    "            for i, chunk in enumerate(row['Chunks']):\n",
    "                chunk_id = f\"{doc_id}_{i}\"\n",
    "                chunk_text = ' '.join(chunk)\n",
    "                chunk_vector = get_embeddings(chunk_text)\n",
    "                index.upsert([(chunk_id, chunk_vector)])\n",
    "\n",
    "# Get the final DataFrame\n",
    "final_df = process_json_files()\n",
    "\n",
    "# Upload the DataFrame to Pinecone\n",
    "dimension = 768  # Dimensionality of your embeddings\n",
    "index_name = \"memory-project\"  # Replace with your Pinecone index name\n",
    "upload_to_pinecone(final_df, index_name, dimension)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
