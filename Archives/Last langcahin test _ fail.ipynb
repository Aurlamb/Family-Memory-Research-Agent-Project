{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-community langchain-groq langchain-core langchain-pinecone gpt4all langgraph sentence-transformers gradio langchain-huggingface groq ollama cohere langchain pinecone-client PyPDF2 einops html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \\\n",
    "    datasets==2.19.1 \\\n",
    "    langchain-pinecone==0.1.1 \\\n",
    "    langchain-openai==0.1.9 \\\n",
    "    langchain==0.2.5 \\\n",
    "    langchain-core==0.2.9 \\\n",
    "    langgraph==0.1.1 \\\n",
    "    semantic-router==0.0.48 \\\n",
    "    serpapi==0.1.5 \\\n",
    "    google-search-results==2.4.2 \\\n",
    "    pygraphviz==1.12  # for visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders.web_base import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import nest_asyncio\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langchain.chains.combine_documents import stuff\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from IPython.core.display import Markdown\n",
    "import json\n",
    "import re\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnableBranch,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import validator\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from operator import itemgetter\n",
    "import asyncio\n",
    "import warnings\n",
    "import PyPDF2\n",
    "from typing import overload, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.prebuilt import ToolInvocation, ToolExecutor\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "import operator\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load path from the environment variable\n",
    "env_ih1 = os.getenv(\"ENV_IH1\")\n",
    "\n",
    "dotenv_path = Path(env_ih1)\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY= os.getenv('PINECONE_KEY')\n",
    "SERPAPI_API_KEY = os.getenv('SERPAPI_API_KEY')\n",
    "STEAMSHIP_API_KEY = os.getenv('STEAMSHIP_API_KEY')\n",
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "GEMINI_KEY = os.getenv('GEMINI_KEY')\n",
    "\n",
    "os.environ['PATH'] += os.pathsep + '/usr/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model='text-embedding=embedding-3-small',\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    # device=device -> Not compatible with OpenAI embeddings\n",
    ")\n",
    "\n",
    "vectorstore = PineconeVectorStore(index_name='memory-project', embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow Outputs:\n",
      "{'query': 'Who is Dreyfus?', 'embedding_info': [HumanMessage(content='Generated embedding for query: Who is Dreyfus?', additional_kwargs={}, response_metadata={}, id='efef83d2-1ae8-498d-b6c4-eacbcb8e9f85')], 'results': ['Doc 1: Relevant to Who is Dreyfus?', 'Doc 2: Relevant to Who is Dreyfus?', 'Doc 3: Relevant to Who is Dreyfus?']}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Define the embeddings configuration\n",
    "class Config:\n",
    "    OPENAI_API_KEY = OPENAI_API_KEY  # Replace with your OpenAI API Key\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-embedding-3-small\",\n",
    "    openai_api_key=Config.OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Mocking Pinecone Vector Store\n",
    "class MockVectorStore:\n",
    "    def __init__(self, index_name, embedding):\n",
    "        self.index_name = index_name\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def similarity_search(self, query, k):\n",
    "        # Mock search logic (replace this with actual Pinecone interaction)\n",
    "        return [f\"Doc {i+1}: Relevant to {query}\" for i in range(k)]\n",
    "\n",
    "vectorstore = MockVectorStore(index_name=\"memory-project\", embedding=embedding)\n",
    "\n",
    "# Define the state schema\n",
    "class QueryState(TypedDict):\n",
    "    query: str\n",
    "    embedding_info: Annotated[list, add_messages]\n",
    "    results: list\n",
    "\n",
    "# LangGraph StateGraph workflow\n",
    "memory_saver = MemorySaver()\n",
    "workflow = StateGraph(state_schema=QueryState)\n",
    "\n",
    "# Embedding generation node\n",
    "def generate_embedding(state: QueryState):\n",
    "    # Logic for embedding generation\n",
    "    return {\"embedding_info\": f\"Generated embedding for query: {state['query']}\"}\n",
    "\n",
    "# Vector store similarity search node\n",
    "def perform_similarity_search(state: QueryState):\n",
    "    query = state[\"query\"]\n",
    "    k = 3\n",
    "    results = vectorstore.similarity_search(query, k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "# Adding nodes to the workflow\n",
    "workflow.add_node(\"embedding\", generate_embedding)\n",
    "workflow.add_node(\"similarity_search\", perform_similarity_search)\n",
    "\n",
    "# Connecting nodes\n",
    "workflow.add_edge(START, \"embedding\")\n",
    "workflow.add_edge(\"embedding\", \"similarity_search\")\n",
    "workflow.add_edge(\"similarity_search\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "compiled_workflow = workflow.compile(checkpointer=memory_saver)\n",
    "\n",
    "# Example usage with a query\n",
    "input_state = {\"query\": \"Who is Dreyfus?\"}\n",
    "\n",
    "# Provide a configuration with the required keys\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"example-thread-1\",  # Unique identifier for this workflow run\n",
    "        \"checkpoint_ns\": \"\",  # Optional namespace, can be left as an empty string\n",
    "        \"checkpoint_id\": None,  # Optional checkpoint ID to resume from, use None for a fresh run\n",
    "    }\n",
    "}\n",
    "\n",
    "# Invoke the workflow\n",
    "outputs = compiled_workflow.invoke(input_state, config=config)\n",
    "\n",
    "print(\"Workflow Outputs:\")\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
