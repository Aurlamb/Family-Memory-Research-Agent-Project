{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Memory Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load path from the environment variable\n",
    "env_ih1 = os.getenv(\"ENV_IH1\")\n",
    "\n",
    "dotenv_path = Path(env_ih1)\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY= os.getenv('PINECONE_KEY')\n",
    "SERPAPI_API_KEY = os.getenv('SERPAPI_API_KEY')\n",
    "STEAMSHIP_API_KEY = os.getenv('STEAMSHIP_API_KEY')\n",
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "GEMINI_KEY = os.getenv('GEMINI_KEY')\n",
    "\n",
    "os.environ['PATH'] += os.pathsep + '/usr/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import wrappers, traceable\n",
    "\n",
    "LANGSMITH_API_KEY= LANGSMITH_API_KEY\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"memory-project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pinecone DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 1014}},\n",
       " 'total_vector_count': 1014}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Connect to the existing index\n",
    "index_name = \"memory-project3\"  # Replace with your existing index name\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aurel\\AppData\\Local\\Temp\\ipykernel_17280\\1506975490.py:16: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)\n",
      "C:\\Users\\aurel\\AppData\\Local\\Temp\\ipykernel_17280\\1506975490.py:19: LangChainDeprecationWarning: The class `Pinecone` was deprecated in LangChain 0.0.18 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-pinecone package and should be used instead. To use it run `pip install -U :class:`~langchain-pinecone` and import as `from :class:`~langchain_pinecone import Pinecone``.\n",
      "  vectorstore = LangchainPinecone(index, embeddings, text_key=\"text\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "## Create encoder\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "\n",
    "encoder = OpenAIEncoder(\n",
    "    name=\"text-embedding-3-small\",\n",
    "    openai_api_key=OPENAI_API_KEY \n",
    ")\n",
    "\n",
    "## Creating retriever\n",
    "\n",
    "# Initialize OpenAI Embeddings with text-embedding-3-small\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize LangChain Pinecone retriever\n",
    "vectorstore = LangchainPinecone(index, embeddings, text_key=\"text\")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aurel\\AppData\\Local\\Temp\\ipykernel_17280\\3093734283.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: POUR LA MÉMOIRE\n",
      "FAMILIALE\n",
      "\n",
      "FAMILLE HISTOIRE\n",
      "SOUVENIRS & COMMENTAIRES\n",
      "\n",
      "VOLUME 1\n",
      "\n",
      "Jean-Georges Lambert\n",
      "Avril 1993\n",
      "Metadata: {'Author': 'Jean Lambert', 'Chunk_ID': 'Pour la mémoire familiale 1-50_Chunk1', 'Doc name': 'Pour la mémoire familiale 1-50', 'Page_number': 1.0, 'Total_Chunks': 1.0}\n",
      "--------------------------------------------------\n",
      "Content: C'est le dernier, Isaac, dit Hovel, le plus souvent appelé Louis qui est notre ancêtre direct à la génération suivante.\n",
      "\n",
      "Il est né en 1766 à Froeningen, et s'est marié avec Rachel (ou Reiche ou Rosalie ou Thérèse) Gugenheim, née également à Froeningen en 1772, et décédée dans le même village en 1843, à 81 ans.\n",
      "\n",
      "Du ménage Louis et Rachel, je connais six enfants: Charlotte ex Judele, Jacques ex Jacob, marchand de bestiaux, Alexandre ex Samuel, marchand de bestiaux, Lehmann ou Clément, Marx, marchand de bétail, et Julie ex Sara, tous nés à Froeningen entre 1795 et 1802.\n",
      "Metadata: {'Author': 'John Doe', 'Chunk_ID': 'Pdf img_Chunk1', 'Doc name': 'Pdf img', 'Page_number': 118.0, 'Total_Chunks': 8.0}\n",
      "--------------------------------------------------\n",
      "Content: C'est le dernier, Isaac, dit Hovel, le plus souvent appelé Louis qui est notre ancêtre direct à la génération suivante.\n",
      "\n",
      "Il est né en 1766 à Froeningen, et s'est marié avec Rachel (ou Reiche ou Rosalie ou Thérèse) Gugenheim, née également à Froeningen en 1772, et décédée dans le même village en 1843, à 81 ans.\n",
      "\n",
      "Du ménage Louis et Rachel, je connais six enfants: Charlotte ex Judele, Jacques ex Jacob, marchand de bestiaux, Alexandre ex Samuel, marchand de bestiaux, Lehmann ou Clément, Marx, marchand de bétail, et Julie ex Sara, tous nés à Froeningen entre 1795 et 1802.\n",
      "Metadata: {'Author': 'John Doe', 'Chunk_ID': 'Pdf img_Chunk1', 'Doc name': 'Pdf img', 'Page_number': 118.0, 'Total_Chunks': 8.0}\n",
      "--------------------------------------------------\n",
      "Content: C'est le dernier, Isaac, dit Hovel, le plus souvent appelé Louis qui est notre ancêtre direct à la génération suivante.\n",
      "\n",
      "Il est né en 1766 à Froeningen, et s'est marié avec Rachel (ou Reiche ou Rosalie ou Thérèse) Gugenheim, née également à Froeningen en 1772, et décédée dans le même village en 1843, à 81 ans.\n",
      "\n",
      "Du ménage Louis et Rachel, je connais six enfants: Charlotte ex Judele, Jacques ex Jacob, marchand de bestiaux, Alexandre ex Samuel, marchand de bestiaux, Lehmann ou Clément, Marx, marchand de bétail, et Julie ex Sara, tous nés à Froeningen entre 1795 et 1802.\n",
      "Metadata: {'Author': 'John Doe', 'Chunk_ID': 'Pdf img_Chunk1', 'Doc name': 'Pdf img', 'Page_number': 118.0, 'Total_Chunks': 8.0}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "query = \"Who is Jean Lambert?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Print results\n",
    "for doc in results:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pformat\n",
    "import ast\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "# Defining Tools\n",
    "##################################################################################\n",
    "\n",
    "def format_rag_contexts(matches: list):\n",
    "    \"\"\"Formats retrieved document matches into a readable context string.\"\"\"\n",
    "    contexts = []\n",
    "    for x in matches:\n",
    "        metadata = x.get(\"metadata\", {})  # Safely get metadata\n",
    "        text = (\n",
    "            f\"Doc name: {metadata.get('Doc name', 'N/A')}\\n\"\n",
    "            f\"Author: {metadata.get('Author', 'N/A')}\\n\"\n",
    "            f\"Chunk_ID: {metadata.get('Chunk_ID', 'N/A')}\\n\"\n",
    "            f\"Content: {metadata.get('text', 'No content available')}\"  # Fixed page content reference\n",
    "        )\n",
    "        contexts.append(text)\n",
    "\n",
    "    return \"\\n---\\n\".join(contexts)\n",
    "\n",
    "\n",
    "def translate_to_french(query: str):\n",
    "    \"\"\"Translates the given query to French using OpenAI's `o4-mini` model to improve retriebal in French database.\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",  # \"o4-mini\" may refer to \"gpt-4o\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a translation assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Translate the following text to French:\\n{query}\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "    \n",
    "\n",
    "from typing import List, Union\n",
    "from langchain.tools import tool\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def rag_search(query: str):\n",
    "    \"\"\"Finds related information using a natural language query in the family history.\"\"\"\n",
    "\n",
    "    xq = encoder([query])\n",
    "    xc = index.query(vector=xq, top_k=3, include_metadata=True)\n",
    "\n",
    "    if \"matches\" not in xc or not xc[\"matches\"]:\n",
    "        return \"⚠ No relevant documents found.\"\n",
    "\n",
    "    # Extract best match details\n",
    "    best_match = xc[\"matches\"][0]\n",
    "    chunk_id = best_match[\"metadata\"].get(\"Chunk_ID\", \"\")\n",
    "    doc_name = best_match[\"metadata\"].get(\"Doc name\", \"\")\n",
    "    total_chunks = best_match[\"metadata\"].get(\"Total_Chunks\", 1)  # Default to 1 if missing\n",
    "\n",
    "    try:\n",
    "        total_chunks = int(float(total_chunks))  # Ensure integer conversion from float or string\n",
    "    except (ValueError, TypeError):\n",
    "        total_chunks = 1  # Fallback in case of conversion error\n",
    "\n",
    "    print(f\"🔍 Best Match - Doc: {doc_name}, Chunk: {chunk_id}, Total Chunks: {total_chunks}\")\n",
    "\n",
    "    # Extract chunk number\n",
    "    match = re.match(r\"(.+)_Chunk(\\d+)\", chunk_id)\n",
    "    if not match:\n",
    "        return format_rag_contexts(xc[\"matches\"])  # Return base context if extraction fails\n",
    "\n",
    "    _, chunk_num = match.groups()\n",
    "\n",
    "    try:\n",
    "        chunk_num = int(chunk_num)  # Ensure chunk_num is an integer\n",
    "    except ValueError:\n",
    "        return format_rag_contexts(xc[\"matches\"])  # Fallback in case of error\n",
    "\n",
    "    # Debug: Print extracted chunk numbers before range calculation\n",
    "    print(f\"🔢 Extracted Chunk Number: {chunk_num}, Total Chunks: {total_chunks}\")\n",
    "\n",
    "    # Generate **bounded** chunk IDs within valid range\n",
    "    min_chunk = max(1, chunk_num - 3)\n",
    "    max_chunk = min(total_chunks, chunk_num + 3)\n",
    "\n",
    "    # Debug: Check type before using in range\n",
    "    print(f\"🔍 Before Casting - min_chunk: {min_chunk} ({type(min_chunk)}), max_chunk: {max_chunk} ({type(max_chunk)})\")\n",
    "\n",
    "    # Ensure integer conversion\n",
    "    min_chunk = int(min_chunk)\n",
    "    max_chunk = int(max_chunk)\n",
    "\n",
    "    # Debug: Confirm type after casting\n",
    "    print(f\"✅ After Casting - min_chunk: {min_chunk} ({type(min_chunk)}), max_chunk: {max_chunk} ({type(max_chunk)})\")\n",
    "\n",
    "    expanded_chunk_ids = [f\"{doc_name}_Chunk{i}\" for i in range(min_chunk, max_chunk + 1)]\n",
    "\n",
    "    print(f\"🧩 Expanding context with bounded chunks: {expanded_chunk_ids}\")\n",
    "\n",
    "    # Retrieve expanded chunks\n",
    "    expanded_contexts = []\n",
    "    for expanded_chunk in expanded_chunk_ids:\n",
    "        try:\n",
    "            result = chunk_search.run(expanded_chunk)\n",
    "            if result and \"⚠ No context found\" not in result:\n",
    "                expanded_contexts.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error retrieving chunk {expanded_chunk}: {str(e)}\")\n",
    "\n",
    "    # Combine and return formatted context\n",
    "    full_context = \"\\n\\n\".join(expanded_contexts) if expanded_contexts else format_rag_contexts(xc[\"matches\"])\n",
    "    return full_context\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tool for chunk search\n",
    "def chunk_search(query: str, chunk_id: str):\n",
    "    \"\"\"Finds related information based on the chunk_id. Helps to get more context.\"\"\"\n",
    "\n",
    "    # Extract Doc Name and Chunk Number from chunk_id\n",
    "    match = re.match(r\"(.+)_Chunk(\\d+)\", chunk_id)\n",
    "    if not match:\n",
    "        return f\"❌ Error: Invalid chunk_id format '{chunk_id}'\"\n",
    "\n",
    "    doc_name, chunk_number = match.groups()\n",
    "    chunk_number = int(chunk_number)\n",
    "\n",
    "    # Generate surrounding chunk IDs (-2, -1, +1, +2)\n",
    "    chunk_ids = [f\"{doc_name}_Chunk{i}\" for i in range(chunk_number - 10, chunk_number + 10) if i > 0]\n",
    "\n",
    "    print(f\"🔍 Debugging Chunk IDs: {chunk_ids}\")\n",
    "\n",
    "    # Encode query into a vector (Replace `encoder()` with your actual embedding function)\n",
    "    xq = encoder([query])  # Convert query into vector\n",
    "\n",
    "    try:\n",
    "        # Perform a **vector-based search** instead of only filtering by chunk ID\n",
    "        response = index.query(\n",
    "            vector=xq,\n",
    "            namespace=\"\",\n",
    "            filter={\"Chunk_ID\": {\"$in\": chunk_ids}},  # Ensure correct filter usage\n",
    "            top_k=5,  # Fetch relevant results\n",
    "            include_metadata=True\n",
    "        )\n",
    "        direct_matches = response[\"matches\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"❌ Pinecone Query Error: {str(e)}\"\n",
    "\n",
    "    # Format and return results\n",
    "    results = []\n",
    "    for match in direct_matches:\n",
    "        metadata = match.get(\"metadata\", {})\n",
    "        chunk_text = metadata.get(\"text\", \"No content available\")\n",
    "        chunk_id_match = metadata.get(\"Chunk_ID\", \"Unknown Chunk\")\n",
    "        results.append(f\"📜 Found Chunk: {chunk_id_match} -> {chunk_text}...\")  # Limit text preview\n",
    "\n",
    "    return \"\\n\".join(results) if results else \"⚠ No relevant chunks found.\"\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def final_answer(answer: str, explore_next: str = \"Would you like to ask about another topic?\", sources: str = None):\n",
    "    \"\"\"\n",
    "    Formats the final answer using a structured approach.\n",
    "\n",
    "    Args:\n",
    "        answer (str): The main response.\n",
    "        explore_next (str, optional): Suggested next question.\n",
    "        sources (str, optional): Source references.\n",
    "\n",
    "    Returns:\n",
    "        str: A well-structured response.\n",
    "    \"\"\"\n",
    "\n",
    "    # 🔹 Construct sources section only if sources are available\n",
    "    sources_section = f\"- **Sources**: {sources}\" if sources and sources.lower() != 'none' else \"\"\n",
    "\n",
    "    # 🔹 Define the structured prompt within the function\n",
    "    final_answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are an intelligent assistant helping with historical research.\n",
    "    Format the final answer for the user by including:\n",
    "\n",
    "    - **Answer**: {answer}\n",
    "    {sources_section}\n",
    "    - **Explore Next**: {explore_next}\n",
    "\n",
    "    Ensure the response is clear and well-structured.\n",
    "    \"\"\")\n",
    "\n",
    "    # 🔹 Ensure all fields are properly formatted before passing to the prompt\n",
    "    structured_input = {\n",
    "        \"answer\": answer,\n",
    "        \"explore_next\": explore_next if explore_next else \"No further suggestions.\",\n",
    "        \"sources_section\": sources_section\n",
    "    }\n",
    "\n",
    "    # 🔹 Use the embedded prompt template to format the response\n",
    "    formatted_response = final_answer_prompt.format(**structured_input)\n",
    "\n",
    "    return formatted_response  # ✅ Always returns a well-structured response\n",
    "\n",
    "\n",
    "\n",
    "# Binding tools to the LLM\n",
    "##################################################################################\n",
    "\n",
    "# Create tool bindings with additional attributes\n",
    "from langchain.tools import Tool\n",
    "\n",
    "# Tool for formatting RAG contexts\n",
    "format_rag_contexts_tool = Tool.from_function(\n",
    "    func=format_rag_contexts,\n",
    "    name=\"format_rag_contexts\",\n",
    "    description=\"Formats retrieved document matches into a readable context string.\",\n",
    "    return_direct=False\n",
    ")\n",
    "\n",
    "# Tool for translating text to French\n",
    "translate_to_french_tool = Tool.from_function(\n",
    "    func=translate_to_french,\n",
    "    name=\"translate_to_french\",\n",
    "    description=\"Translates the given query to French using OpenAI's GPT-4o model to improve retrieval in a French database.\",\n",
    "    return_direct=False\n",
    ")\n",
    "\n",
    "# Tool for RAG search\n",
    "rag_search_tool = Tool.from_function(\n",
    "    func=rag_search,\n",
    "    name=\"rag_search\",\n",
    "    description=\"Finds related information using a natural language query in the family history database.\",\n",
    "    return_direct=False\n",
    ")\n",
    "\n",
    "# Tool for chunk search\n",
    "chunk_search_tool = Tool.from_function(\n",
    "    func=chunk_search,\n",
    "    name=\"chunk_search\",\n",
    "    description=\"Finds related information based on the chunk_id, helping to get more context around a given chunk.\",\n",
    "    return_direct=False\n",
    ")\n",
    "\n",
    "# Tool for final answer generation\n",
    "final_answer_tool = Tool.from_function(\n",
    "    func=final_answer,\n",
    "    name=\"final_answer\",\n",
    "    description=(\n",
    "        \"Generates a natural language response to the user's question based on the family memory database. \"\n",
    "        \"Links information together from multiple sources but does not invent new information. \"\n",
    "        \"If no answer is found, it explicitly states that it doesn't know.\"\n",
    "    ),\n",
    "    return_direct=False\n",
    ")\n",
    "\n",
    "# List of all tools\n",
    "toolbox = [\n",
    "    format_rag_contexts_tool,\n",
    "    translate_to_french_tool,\n",
    "    rag_search_tool,\n",
    "    chunk_search_tool,\n",
    "    final_answer_tool\n",
    "]\n",
    "\n",
    "\n",
    "# OPENAI_API_KEY environment variable must be set\n",
    "simple_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_with_tools = simple_llm.bind_tools(toolbox)\n",
    "\n",
    "\n",
    "# Defining Agent's node\n",
    "##################################################################################\n",
    "\n",
    "# System message\n",
    "assistant_system_message = SystemMessage(content=(\"\"\"You are the Family Safe, keeper of the family's collective memory. \n",
    "Your role is to decide how to handle user queries using the available tools.\n",
    "\n",
    "**Tool Usage:**\n",
    "- Do NOT reuse a tool for the same query (check the scratchpad).\n",
    "- Do NOT use any tool more than **3 times**.\n",
    "- Prioritize **rag_search** for gathering information.\n",
    "- Use **chunk_search** to find context around a specific chunk.\n",
    "- Do not mix sources from different contexts unless necessary.\n",
    "- Alsways check at leats 2 ***Doc name*** to ensure the information is correct.\n",
    "\n",
    "**Response Protocol:**\n",
    "- If tools provides no answer, state that you don't know or can't any information about this topic\"\n",
    "- NEVER invent information or use data beyond the family memory.\n",
    "- Always provide sources via the **final_answer** tool.\n",
    "- Chunk_search must be in the scracthpad to point to final_answer.\n",
    "- Discard any page content that looks like a table of content: you won't find any useful information there apart from page numbers.\n",
    "\n",
    "By following these rules, you ensure accurate and responsible responses.\"\"\"))\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "   return {\"messages\": [llm_with_tools.invoke([assistant_system_message] + state[\"messages\"])]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Defining Graph\n",
    "##################################################################################\n",
    "\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(toolbox))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "react_graph_with_memory = builder.compile(checkpointer=memory)\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph_with_memory.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "# Define oracle function\n",
    "################################################################################################\n",
    "\n",
    "# Specify a thread\n",
    "def oracle(user_request: str, thread_id = \"1\", verbose = False):\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    messages = react_graph_with_memory.invoke({\"messages\": [HumanMessage(content=user_request)]}, config)\n",
    "    if verbose:\n",
    "        for message in messages['messages']:\n",
    "            message.pretty_print()\n",
    "    else:\n",
    "        messages['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Best Match - Doc: Pour la mémoire familiale 1-50, Chunk: Pour la mémoire familiale 1-50_Chunk5, Total Chunks: 6\n",
      "🔢 Extracted Chunk Number: 5, Total Chunks: 6\n",
      "🔍 Before Casting - min_chunk: 2 (<class 'int'>), max_chunk: 6 (<class 'int'>)\n",
      "✅ After Casting - min_chunk: 2 (<class 'int'>), max_chunk: 6 (<class 'int'>)\n",
      "🧩 Expanding context with bounded chunks: ['Pour la mémoire familiale 1-50_Chunk2', 'Pour la mémoire familiale 1-50_Chunk3', 'Pour la mémoire familiale 1-50_Chunk4', 'Pour la mémoire familiale 1-50_Chunk5', 'Pour la mémoire familiale 1-50_Chunk6']\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk2: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk3: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk4: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk5: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk6: 'function' object has no attribute 'run'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Answer**: I don't have specific information about his mother in the family memory. However, there is a chapter titled 'De sa naissance à la mort de sa mère (1877-1895)' which may provide insights regarding her. Unfortunately, I cannot retrieve additional details at this moment.\n",
      "\n",
      "**Explore Next**: Would you like to ask about another topic?\n"
     ]
    }
   ],
   "source": [
    "oracle(\n",
    "    user_request=\"Yes, tell me more about his mother.\", \n",
    "    thread_id=\"3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Best Match - Doc: Pour la mémoire familiale 1-50, Chunk: Pour la mémoire familiale 1-50_Chunk1, Total Chunks: 1\n",
      "🔢 Extracted Chunk Number: 1, Total Chunks: 1\n",
      "🔍 Before Casting - min_chunk: 1 (<class 'int'>), max_chunk: 1 (<class 'int'>)\n",
      "✅ After Casting - min_chunk: 1 (<class 'int'>), max_chunk: 1 (<class 'int'>)\n",
      "🧩 Expanding context with bounded chunks: ['Pour la mémoire familiale 1-50_Chunk1']\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk1: 'function' object has no attribute 'run'\n",
      "🔍 Best Match - Doc: Pour la mémoire familiale 1-50, Chunk: Pour la mémoire familiale 1-50_Chunk1, Total Chunks: 1\n",
      "🔢 Extracted Chunk Number: 1, Total Chunks: 1\n",
      "🔍 Before Casting - min_chunk: 1 (<class 'int'>), max_chunk: 1 (<class 'int'>)\n",
      "✅ After Casting - min_chunk: 1 (<class 'int'>), max_chunk: 1 (<class 'int'>)\n",
      "🧩 Expanding context with bounded chunks: ['Pour la mémoire familiale 1-50_Chunk1']\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk1: 'function' object has no attribute 'run'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Answer**: I couldn't find any specific information about when Jean Lambert started to write the book \"Pour la mémoire familiale\".\n",
      "\n",
      "**Explore Next**: Would you like to ask about another topic?\n"
     ]
    }
   ],
   "source": [
    "oracle(\n",
    "    user_request=\"When did Jean start to write this book?\", \n",
    "    thread_id=\"5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Best Match - Doc: Pour la mémoire familiale 1-50, Chunk: Pour la mémoire familiale 1-50_Chunk1, Total Chunks: 1\n",
      "🔢 Extracted Chunk Number: 1, Total Chunks: 1\n",
      "🔍 Before Casting - min_chunk: 1 (<class 'int'>), max_chunk: 1 (<class 'int'>)\n",
      "✅ After Casting - min_chunk: 1 (<class 'int'>), max_chunk: 1 (<class 'int'>)\n",
      "🧩 Expanding context with bounded chunks: ['Pour la mémoire familiale 1-50_Chunk1']\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk1: 'function' object has no attribute 'run'\n",
      "🔍 Best Match - Doc: Pour la mémoire familiale 1-50, Chunk: Pour la mémoire familiale 1-50_Chunk1, Total Chunks: 1\n",
      "🔢 Extracted Chunk Number: 1, Total Chunks: 1\n",
      "🔍 Before Casting - min_chunk: 1 (<class 'int'>), max_chunk: 1 (<class 'int'>)\n",
      "✅ After Casting - min_chunk: 1 (<class 'int'>), max_chunk: 1 (<class 'int'>)\n",
      "🧩 Expanding context with bounded chunks: ['Pour la mémoire familiale 1-50_Chunk1']\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk1: 'function' object has no attribute 'run'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Answer**: Jean Lambert's mother passed away on March 29, 1963.\n",
      "\n",
      "**Explore Next**: Would you like to ask about another topic?\n"
     ]
    }
   ],
   "source": [
    "oracle(\n",
    "    user_request=\"When did Jean's mother died?\", \n",
    "    thread_id=\"5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Best Match - Doc: Pour la mémoire familiale 1-50, Chunk: Pour la mémoire familiale 1-50_Chunk3, Total Chunks: 3\n",
      "🔢 Extracted Chunk Number: 3, Total Chunks: 3\n",
      "🔍 Before Casting - min_chunk: 1 (<class 'int'>), max_chunk: 3 (<class 'int'>)\n",
      "✅ After Casting - min_chunk: 1 (<class 'int'>), max_chunk: 3 (<class 'int'>)\n",
      "🧩 Expanding context with bounded chunks: ['Pour la mémoire familiale 1-50_Chunk1', 'Pour la mémoire familiale 1-50_Chunk2', 'Pour la mémoire familiale 1-50_Chunk3']\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk1: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk2: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk3: 'function' object has no attribute 'run'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Answer**: Le 29 mars 1963, la mère de Jean Lambert est décédée. À cette époque, William n'était pas encore né, et Georges avait treize ans.\n",
      "\n",
      "**Explore Next**: Would you like to ask about another topic?\n"
     ]
    }
   ],
   "source": [
    "oracle(\n",
    "    user_request=\"What happened on March 29, 1963?\", \n",
    "    thread_id=\"7\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Best Match - Doc: Pdf img, Chunk: Pdf img_Chunk6, Total Chunks: 8\n",
      "🔢 Extracted Chunk Number: 6, Total Chunks: 8\n",
      "🔍 Before Casting - min_chunk: 3 (<class 'int'>), max_chunk: 8 (<class 'int'>)\n",
      "✅ After Casting - min_chunk: 3 (<class 'int'>), max_chunk: 8 (<class 'int'>)\n",
      "🧩 Expanding context with bounded chunks: ['Pdf img_Chunk3', 'Pdf img_Chunk4', 'Pdf img_Chunk5', 'Pdf img_Chunk6', 'Pdf img_Chunk7', 'Pdf img_Chunk8']\n",
      "⚠ Error retrieving chunk Pdf img_Chunk3: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pdf img_Chunk4: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pdf img_Chunk5: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pdf img_Chunk6: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pdf img_Chunk7: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pdf img_Chunk8: 'function' object has no attribute 'run'\n",
      "🔍 Best Match - Doc: Pdf img, Chunk: Pdf img_Chunk1, Total Chunks: 8\n",
      "🔢 Extracted Chunk Number: 1, Total Chunks: 8\n",
      "🔍 Before Casting - min_chunk: 1 (<class 'int'>), max_chunk: 4 (<class 'int'>)\n",
      "✅ After Casting - min_chunk: 1 (<class 'int'>), max_chunk: 4 (<class 'int'>)\n",
      "🧩 Expanding context with bounded chunks: ['Pdf img_Chunk1', 'Pdf img_Chunk2', 'Pdf img_Chunk3', 'Pdf img_Chunk4']\n",
      "⚠ Error retrieving chunk Pdf img_Chunk1: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pdf img_Chunk2: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pdf img_Chunk3: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pdf img_Chunk4: 'function' object has no attribute 'run'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Answer**: Je ne sais pas combien d'enfants Jean a.\n",
      "\n",
      "**Explore Next**: Would you like to ask about another topic?\n"
     ]
    }
   ],
   "source": [
    "oracle(\n",
    "    user_request=\"Combien d'enfant a Jean?\", \n",
    "    thread_id=\"9\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It seems there is a technical issue with retrieving information from chunks 7 and 8. I am unable to check for details about sons in those chunks at this moment. \n",
      "\n",
      "If you have other topics or specific names you're interested in, please let me know!\n"
     ]
    }
   ],
   "source": [
    "oracle(\n",
    "    user_request=\"Can you check for sons in chunk 7 & 8?\", \n",
    "    thread_id=\"7\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Answer**: In the family's collective memory documented by Jean Lambert, there is mention of his father's perspective on the Dreyfus affair, which highlights the contextual antisemitism prevalent at the time. Jean's reflections seem to intertwine personal experiences and the larger societal issues surrounding the Dreyfus affair, shedding light on how his family viewed the events and their impact. Specific opinions or feelings expressed by Jean regarding the Dreyfus case may not be detailed, but there is an acknowledgment of the affair's significance and the resultant changes in public discourse about antisemitism.\n",
      "\n",
      "**Explore Next**: Would you like to ask about another topic?\n"
     ]
    }
   ],
   "source": [
    "oracle(\n",
    "    user_request=\"Who are Jean's sons?\", \n",
    "    thread_id=\"8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Best Match - Doc: Pour la mémoire familiale 1-50, Chunk: Pour la mémoire familiale 1-50_Chunk3, Total Chunks: 3\n",
      "🔢 Extracted Chunk Number: 3, Total Chunks: 3\n",
      "🔍 Before Casting - min_chunk: 1 (<class 'int'>), max_chunk: 3 (<class 'int'>)\n",
      "✅ After Casting - min_chunk: 1 (<class 'int'>), max_chunk: 3 (<class 'int'>)\n",
      "🧩 Expanding context with bounded chunks: ['Pour la mémoire familiale 1-50_Chunk1', 'Pour la mémoire familiale 1-50_Chunk2', 'Pour la mémoire familiale 1-50_Chunk3']\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk1: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk2: 'function' object has no attribute 'run'\n",
      "⚠ Error retrieving chunk Pour la mémoire familiale 1-50_Chunk3: 'function' object has no attribute 'run'\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Answer**: There is no information found regarding whether William has any other brothers apart from Georges. The available details only mention Georges as William's brother.\n",
      "\n",
      "**Explore Next**: Would you like to ask about another topic?\n"
     ]
    }
   ],
   "source": [
    "oracle(\n",
    "    user_request=\"What can you tell me about Jacques Drefyus?\", \n",
    "    thread_id=\"10\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
