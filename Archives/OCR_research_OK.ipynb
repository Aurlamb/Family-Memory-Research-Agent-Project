{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load path from the environment variable\n",
    "env_ih1 = os.getenv(\"ENV_IH1\")\n",
    "\n",
    "dotenv_path = Path(env_ih1)\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY= os.getenv('PINECONE_KEY')\n",
    "SERPAPI_API_KEY = os.getenv('SERPAPI_API_KEY')\n",
    "STEAMSHIP_API_KEY = os.getenv('STEAMSHIP_API_KEY')\n",
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "GEMINI_KEY = os.getenv('GEMINI_KEY')\n",
    "\n",
    "os.environ['PATH'] += os.pathsep + '/usr/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytesseract in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (10.4.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fpdf in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (1.7.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (1.60.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.10.5)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# # This notebook requires Tesseract for OCR\n",
    "%pip install pytesseract\n",
    "\n",
    "%pip install fpdf\n",
    "\n",
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page scanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytesseract import pytesseract\n",
    "\n",
    "# Page number position\n",
    "def extract_page_region(image, location='top'):\n",
    "    height, width = image.shape[:2]\n",
    "    region = None\n",
    "\n",
    "    if location == 'top':\n",
    "        region = image[0:int(height * 0.1), :]  # Top 10% of the image\n",
    "    elif location == 'bottom':\n",
    "        region = image[int(height * 0.9):, :]  # Bottom 10% of the image\n",
    "\n",
    "    return region\n",
    "\n",
    "\n",
    "# Extract page number & text\n",
    "def extract_text_with_page_number(image, page_location='top'):\n",
    "    # Extract the main text\n",
    "    full_text = pytesseract.image_to_string(image)\n",
    "\n",
    "    # Extract the page number region if applicable\n",
    "    if page_location in ['top', 'bottom']:\n",
    "        page_region = extract_page_region(image, location=page_location)\n",
    "        page_text = pytesseract.image_to_string(page_region)\n",
    "    else:\n",
    "        page_text = ''\n",
    "\n",
    "    # Extract the page number using regex (look for digits)\n",
    "    import re\n",
    "    page_number_match = re.search(r'\\b\\d+\\b', page_text)\n",
    "    page_number = int(page_number_match.group()) if page_number_match else None\n",
    "\n",
    "    return full_text, page_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create assembled document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Images with Page Location Parameter\n",
    "\n",
    "def process_images(image_paths, page_location='top'):\n",
    "    pages = []\n",
    "    for path in image_paths:\n",
    "        preprocessed = preprocess_image(path)\n",
    "        text, page_number = extract_text_with_page_number(preprocessed, page_location=page_location)\n",
    "        pages.append((page_number, text))\n",
    "\n",
    "    # Sort pages by number\n",
    "    pages = sorted(pages, key=lambda x: x[0] if x[0] is not None else float('inf'))\n",
    "    return [text for _, text in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "def create_pdf(image_paths, output_path, title=\"Document\", author=\"Unknown\"):\n",
    "    pdf = FPDF()\n",
    "    \n",
    "    # Set metadata\n",
    "    pdf.set_title(title)\n",
    "    pdf.set_author(author)\n",
    "    \n",
    "    # Add a title page\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=16)\n",
    "    pdf.multi_cell(0, 10, f\"{title}\\nby {author}\", align='C')\n",
    "    \n",
    "    # Add images as subsequent pages\n",
    "    for image_path in image_paths:\n",
    "        pdf.add_page()\n",
    "        pdf.image(image_path, x=0, y=0, w=210, h=297)  # A4 dimensions in mm\n",
    "    \n",
    "    pdf.output(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Image folder location\n",
    "\n",
    "def get_image_paths_from_folder(folder_path, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    \"\"\"\n",
    "    Retrieve all image file paths from a folder.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "        extensions (tuple): Allowed image file extensions.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of image file paths.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        os.path.join(folder_path, file)\n",
    "        for file in os.listdir(folder_path)\n",
    "        if file.lower().endswith(extensions)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: '.\\\\Photos\\\\Test set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_folder\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPhotos\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTest set\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Folder containing images\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_text_file\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_pdf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Set to True to enable PDF creation\u001b[39;00m\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Retrieve image paths from the folder\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m \u001b[43mget_image_paths_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_folder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m, in \u001b[0;36mget_image_paths_from_folder\u001b[1;34m(folder_path, extensions)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_image_paths_from_folder\u001b[39m(folder_path, extensions\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    Retrieve all image file paths from a folder.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m        list: List of image file paths.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     17\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file)\n\u001b[1;32m---> 18\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(extensions)\n\u001b[0;32m     20\u001b[0m     ]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Le chemin d’accès spécifié est introuvable: '.\\\\Photos\\\\Test set'"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'image_folder': '.\\Photos\\Test set',  # Folder containing images\n",
    "    'output_text_file': 'output.txt',\n",
    "    'output_pdf_file': 'output.pdf',  # Optional, only needed if create_pdf=True\n",
    "    'title': 'Test',\n",
    "    'author': 'John Doe',\n",
    "    'page_location': 'bottom',  # Options: 'top', 'bottom', 'none'\n",
    "    'create_pdf': False,  # Set to True to enable PDF creation\n",
    "}\n",
    "\n",
    "# Retrieve image paths from the folder\n",
    "image_paths = get_image_paths_from_folder(config['image_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    image_paths, \n",
    "    output_text_file, \n",
    "    output_pdf_file=None, \n",
    "    title=\"Document\", \n",
    "    author=\"Unknown\", \n",
    "    page_location=\"top\", \n",
    "    create_pdf=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to process images, extract text, and optionally create a PDF.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of image file paths.\n",
    "        output_text_file (str): Path to save the extracted text file.\n",
    "        output_pdf_file (str): Path to save the PDF (if create_pdf is True).\n",
    "        title (str): Title of the document.\n",
    "        author (str): Author of the document.\n",
    "        page_location (str): Location of page numbers ('top', 'bottom', 'none').\n",
    "        create_pdf (bool): Whether to create a PDF.\n",
    "    \"\"\"\n",
    "    # Process images and extract text\n",
    "    texts = process_images(image_paths, page_location=page_location)\n",
    "    \n",
    "    # Save text to a file\n",
    "    save_to_text_file(texts, output_text_file)\n",
    "    print(f\"Text saved to: {output_text_file}\")\n",
    "    \n",
    "    # Optionally create a PDF\n",
    "    if create_pdf:\n",
    "        if not output_pdf_file:\n",
    "            raise ValueError(\"output_pdf_file must be provided when create_pdf is True.\")\n",
    "        create_pdf(image_paths, output_pdf_file, title=title, author=author)\n",
    "        print(f\"PDF saved to: {output_pdf_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call the main function\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_text_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_text_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_pdf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_pdf_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauthor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpage_location\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreate_pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(image_paths, output_text_file, output_pdf_file, title, author, page_location, create_pdf)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mMain function to process images, extract text, and optionally create a PDF.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    create_pdf (bool): Whether to create a PDF.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Process images and extract text\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Save text to a file\u001b[39;00m\n\u001b[0;32m     26\u001b[0m save_to_text_file(texts, output_text_file)\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m, in \u001b[0;36mprocess_images\u001b[1;34m(image_paths, page_location)\u001b[0m\n\u001b[0;32m      4\u001b[0m pages \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m image_paths:\n\u001b[1;32m----> 6\u001b[0m     preprocessed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m(path)\n\u001b[0;32m      7\u001b[0m     text, page_number \u001b[38;5;241m=\u001b[39m extract_text_with_page_number(preprocessed, page_location\u001b[38;5;241m=\u001b[39mpage_location)\n\u001b[0;32m      8\u001b[0m     pages\u001b[38;5;241m.\u001b[39mappend((page_number, text))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_image' is not defined"
     ]
    }
   ],
   "source": [
    "# Call the main function\n",
    "main(\n",
    "    image_paths=image_paths,\n",
    "    output_text_file=config['output_text_file'],\n",
    "    output_pdf_file=config['output_pdf_file'],\n",
    "    title=config['title'],\n",
    "    author=config['author'],\n",
    "    page_location=config['page_location'],\n",
    "    create_pdf=config['create_pdf']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annex - Full python script with PDF export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to: output.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import cv2\n",
    "from pytesseract import pytesseract\n",
    "from fpdf import FPDF\n",
    "\n",
    "\n",
    "# Function to retrieve all image paths from a folder\n",
    "def get_image_paths_from_folder(folder_path, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    return [\n",
    "        os.path.join(folder_path, file)\n",
    "        for file in os.listdir(folder_path)\n",
    "        if file.lower().endswith(extensions)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Preprocess image for better OCR results\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "    return thresh\n",
    "\n",
    "\n",
    "# Extract the page region based on the location parameter\n",
    "def extract_page_region(image, location='top'):\n",
    "    height, width = image.shape[:2]\n",
    "    if location == 'top':\n",
    "        return image[0:int(height * 0.1), :]  # Top 10%\n",
    "    elif location == 'bottom':\n",
    "        return image[int(height * 0.9):, :]  # Bottom 10%\n",
    "    return None  # If 'none', skip\n",
    "\n",
    "\n",
    "# Extract text and page numbers\n",
    "def extract_text_with_page_number(image, page_location='top'):\n",
    "    # Full text extraction\n",
    "    full_text = pytesseract.image_to_string(image)\n",
    "    \n",
    "    # Extract page number if applicable\n",
    "    page_text = ''\n",
    "    if page_location in ['top', 'bottom']:\n",
    "        page_region = extract_page_region(image, page_location)\n",
    "        if page_region is not None:\n",
    "            page_text = pytesseract.image_to_string(page_region)\n",
    "\n",
    "    # Find page number using regex\n",
    "    page_number_match = re.search(r'\\b\\d+\\b', page_text)\n",
    "    page_number = int(page_number_match.group()) if page_number_match else None\n",
    "\n",
    "    return full_text, page_number\n",
    "\n",
    "\n",
    "# Organize pages based on extracted page numbers\n",
    "def process_images(image_paths, page_location='top'):\n",
    "    pages = []\n",
    "    for path in image_paths:\n",
    "        preprocessed = preprocess_image(path)\n",
    "        text, page_number = extract_text_with_page_number(preprocessed, page_location=page_location)\n",
    "        pages.append((page_number, text))\n",
    "    \n",
    "    # Sort pages by number\n",
    "    pages = sorted(pages, key=lambda x: x[0] if x[0] is not None else float('inf'))\n",
    "    return [text for _, text in pages]\n",
    "\n",
    "\n",
    "# Save extracted text to a file\n",
    "def save_to_text_file(texts, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for page_text in texts:\n",
    "            f.write(page_text + \"\\n\\n\")\n",
    "\n",
    "\n",
    "# Create a PDF with metadata\n",
    "def create_pdf(image_paths, output_path, title=\"Document\", author=\"Unknown\"):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_title(title)\n",
    "    pdf.set_author(author)\n",
    "\n",
    "    # Add a title page\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=16)\n",
    "    pdf.multi_cell(0, 10, f\"{title}\\nby {author}\", align='C')\n",
    "\n",
    "    # Add image pages\n",
    "    for image_path in image_paths:\n",
    "        pdf.add_page()\n",
    "        pdf.image(image_path, x=0, y=0, w=210, h=297)  # A4 dimensions in mm\n",
    "\n",
    "    pdf.output(output_path)\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main(\n",
    "    image_folder,\n",
    "    output_text_file,\n",
    "    output_pdf_file=None,\n",
    "    title=\"Document\",\n",
    "    author=\"Unknown\",\n",
    "    page_location=\"top\",\n",
    "    create_pdf=False\n",
    "):\n",
    "    # Retrieve image paths\n",
    "    image_paths = get_image_paths_from_folder(image_folder)\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in folder: {image_folder}\")\n",
    "        return\n",
    "\n",
    "    # Process images and extract text\n",
    "    texts = process_images(image_paths, page_location=page_location)\n",
    "    \n",
    "    # Save extracted text to a file\n",
    "    save_to_text_file(texts, output_text_file)\n",
    "    print(f\"Text saved to: {output_text_file}\")\n",
    "    \n",
    "    # Optionally create a PDF\n",
    "    if create_pdf:\n",
    "        if not output_pdf_file:\n",
    "            raise ValueError(\"Output PDF file path must be provided when create_pdf is True.\")\n",
    "        create_pdf(image_paths, output_pdf_file, title=title, author=author)\n",
    "        print(f\"PDF saved to: {output_pdf_file}\")\n",
    "\n",
    "\n",
    "# Configuration\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'image_folder': r'.\\Photos\\Test2',  # Replace with your folder path\n",
    "        'output_text_file': 'output.txt',\n",
    "        'output_pdf_file': 'output.pdf',  # Optional, only needed if create_pdf=True\n",
    "        'title': 'Test',\n",
    "        'author': 'Author Name',\n",
    "        'page_location': 'bottom',  # Options: 'top', 'bottom', 'none'\n",
    "        'create_pdf': False,  # Set to True to enable PDF creation\n",
    "    }\n",
    "\n",
    "    main(\n",
    "        image_folder=config['image_folder'],\n",
    "        output_text_file=config['output_text_file'],\n",
    "        output_pdf_file=config['output_pdf_file'],\n",
    "        title=config['title'],\n",
    "        author=config['author'],\n",
    "        page_location=config['page_location'],\n",
    "        create_pdf=config['create_pdf']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (2.24.0)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Using cached google_api_python_client-2.159.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (2.36.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (3.20.2)\n",
      "Requirement already satisfied: pydantic in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (2.10.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.25.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core->google-generativeai) (1.66.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.69.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.69.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.12.14)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Using cached google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Using cached google_api_python_client-2.159.0-py2.py3-none-any.whl (12.8 MB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Installing collected packages: uritemplate, protobuf, httplib2, google-auth-httplib2, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.2\n",
      "    Uninstalling protobuf-3.20.2:\n",
      "      Successfully uninstalled protobuf-3.20.2\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-python-client-2.159.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.4 httplib2-0.22.0 protobuf-5.29.3 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "paddlepaddle 2.6.2 requires protobuf<=3.20.2,>=3.1.0; platform_system == \"Windows\", but you have protobuf 5.29.3 which is incompatible.\n",
      "paddlepaddle-gpu 2.6.2 requires protobuf<=3.20.2,>=3.1.0; platform_system == \"Windows\", but you have protobuf 5.29.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: protobuf in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (5.29.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install google.generativeai\n",
    "\n",
    "# Install the required packages\n",
    "%pip install google-generativeai\n",
    "%pip install --upgrade protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to: output.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Replace with your actual API key\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\") \n",
    "\n",
    "def get_image_paths_from_folder(folder_path, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    return [\n",
    "        os.path.join(folder_path, file)\n",
    "        for file in os.listdir(folder_path)\n",
    "        if file.lower().endswith(extensions)\n",
    "    ]\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    prompt = f\"Extract text from this image: {image_path}\"\n",
    "    generation_config = genai.types.GenerationConfig(\n",
    "        temperature=0.1,  # Adjust as needed\n",
    "        max_output_tokens=1024  # Adjust as needed\n",
    "    )\n",
    "    response = model.generate_content([prompt, image], generation_config=generation_config)\n",
    "    return response.text\n",
    "\n",
    "def process_images(image_paths):\n",
    "    pages = []\n",
    "    for path in image_paths:\n",
    "        text = extract_text_from_image(path)\n",
    "        pages.append(text)\n",
    "    return pages\n",
    "\n",
    "def save_to_text_file(texts, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for page_text in texts:\n",
    "            f.write(page_text + \"\\n\\n\")\n",
    "\n",
    "def main(\n",
    "    image_folder,\n",
    "    output_text_file,\n",
    "):\n",
    "    # Retrieve image paths\n",
    "    image_paths = get_image_paths_from_folder(image_folder)\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in folder: {image_folder}\")\n",
    "        return\n",
    "\n",
    "    # Process images and extract text\n",
    "    texts = process_images(image_paths)\n",
    "\n",
    "    # Save extracted text to a file\n",
    "    save_to_text_file(texts, output_text_file)\n",
    "    print(f\"Text saved to: {output_text_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'image_folder': r'.\\Photos\\Test2',  # Replace with your folder path\n",
    "        'output_text_file': 'output.txt',\n",
    "    }\n",
    "\n",
    "    main(\n",
    "        image_folder=config['image_folder'],\n",
    "        output_text_file=config['output_text_file'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: .\\Photos\\Test_set\\20250122_155459.jpg\n",
      "Processing image: .\\Photos\\Test_set\\20250122_155547.jpg\n",
      "Processing image: .\\Photos\\Test_set\\20250122_155600.jpg\n",
      "Processing image: .\\Photos\\Test_set\\20250122_155618.jpg\n",
      "Processing image: .\\Photos\\Test_set\\20250122_155631.jpg\n",
      "Processing image: .\\Photos\\Test_set\\20250122_155645.jpg\n",
      "Processing image: .\\Photos\\Test_set\\20250122_155651.jpg\n",
      "Text saved to: output_tesseract.txt\n",
      "PDF saved to: output_tesseract.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from fpdf import FPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "# Function to retrieve all image paths from a folder\n",
    "def get_image_paths_from_folder(folder_path, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    \"\"\"\n",
    "    Retrieve all image file paths from a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "        extensions (tuple): Allowed image file extensions.\n",
    "\n",
    "    Returns:\n",
    "        list: List of image file paths.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise ValueError(f\"Folder does not exist: {folder_path}\")\n",
    "\n",
    "    return [\n",
    "        os.path.join(folder_path, file)\n",
    "        for file in os.listdir(folder_path)\n",
    "        if file.lower().endswith(extensions)\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_text_tesseract(image_path):\n",
    "    \"\"\"\n",
    "    Extract text from an image using Tesseract OCR.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def process_images_with_tesseract(image_paths):\n",
    "    \"\"\"\n",
    "    Process images and extract text using Tesseract OCR.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of image file paths.\n",
    "\n",
    "    Returns:\n",
    "        list: Extracted texts in the order of image_paths.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for path in image_paths:\n",
    "        print(f\"Processing image: {path}\")\n",
    "        text = extract_text_tesseract(path)\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def save_to_text_file(texts, output_path):\n",
    "    \"\"\"\n",
    "    Save extracted texts to a text file.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of extracted texts.\n",
    "        output_path (str): Path to the output text file.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for page_text in texts:\n",
    "            f.write(page_text + \"\\n\\n\")\n",
    "\n",
    "\n",
    "def generate_pdf(image_paths, output_path, title=\"Document\", author=\"Unknown\"):\n",
    "    \"\"\"\n",
    "    Create a PDF from images with a title and author.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of image file paths.\n",
    "        output_path (str): Path to save the PDF file.\n",
    "        title (str): Title of the document.\n",
    "        author (str): Author of the document.\n",
    "    \"\"\"\n",
    "    pdf = FPDF()\n",
    "    pdf.set_title(title)\n",
    "    pdf.set_author(author)\n",
    "\n",
    "    # Add a title page\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=16)\n",
    "    pdf.multi_cell(0, 10, f\"{title}\\nby {author}\", align='C')\n",
    "\n",
    "    # Add image pages\n",
    "    for image_path in image_paths:\n",
    "        pdf.add_page()\n",
    "        pdf.image(image_path, x=0, y=0, w=210, h=297)  # A4 dimensions in mm\n",
    "\n",
    "    pdf.output(output_path)\n",
    "\n",
    "\n",
    "def main_with_tesseract(\n",
    "    image_folder,\n",
    "    output_text_file,\n",
    "    output_pdf_file=None,\n",
    "    title=\"Document\",\n",
    "    author=\"Unknown\",\n",
    "    generate_pdf_flag=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to process images, extract text, and optionally create a PDF.\n",
    "\n",
    "    Args:\n",
    "        image_folder (str): Path to the folder containing images.\n",
    "        output_text_file (str): Path to save the extracted text file.\n",
    "        output_pdf_file (str): Path to save the PDF file (if generate_pdf_flag=True).\n",
    "        title (str): Title of the document.\n",
    "        author (str): Author of the document.\n",
    "        generate_pdf_flag (bool): Whether to create a PDF.\n",
    "    \"\"\"\n",
    "    image_paths = get_image_paths_from_folder(image_folder)\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in folder: {image_folder}\")\n",
    "        return\n",
    "\n",
    "    texts = process_images_with_tesseract(image_paths)\n",
    "    save_to_text_file(texts, output_text_file)\n",
    "    print(f\"Text saved to: {output_text_file}\")\n",
    "\n",
    "    if generate_pdf_flag:\n",
    "        if not output_pdf_file:\n",
    "            raise ValueError(\"Output PDF file path must be provided when generate_pdf_flag is True.\")\n",
    "        generate_pdf(image_paths, output_pdf_file, title=title, author=author)\n",
    "        print(f\"PDF saved to: {output_pdf_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'image_folder': r'.\\Photos\\Test2',  # Replace with your folder path\n",
    "        'output_text_file': 'output_tesseract.txt',\n",
    "        'output_pdf_file': 'output_tesseract.pdf',  # Optional, only needed if generate_pdf_flag=True\n",
    "        'title': 'My Book Title',\n",
    "        'author': 'Author Name',\n",
    "        'generate_pdf_flag': True,  # Set to True to enable PDF creation\n",
    "    }\n",
    "\n",
    "    main_with_tesseract(\n",
    "        image_folder=config['image_folder'],\n",
    "        output_text_file=config['output_text_file'],\n",
    "        output_pdf_file=config['output_pdf_file'],\n",
    "        title=config['title'],\n",
    "        author=config['author'],\n",
    "        generate_pdf_flag=config['generate_pdf_flag']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: paddlepaddle in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (2.6.2)\n",
      "Requirement already satisfied: httpx in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.13 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from paddlepaddle) (1.26.4)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle) (10.4.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from paddlepaddle) (5.1.1)\n",
      "Requirement already satisfied: astor in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from paddlepaddle) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from paddlepaddle) (3.3.0)\n",
      "Requirement already satisfied: protobuf<=3.20.2,>=3.1.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from paddlepaddle) (3.20.2)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from httpx->paddlepaddle) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from httpx->paddlepaddle) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: paddlepaddle-gpu in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (2.6.2)\n",
      "Requirement already satisfied: httpx in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle-gpu) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.13 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from paddlepaddle-gpu) (1.26.4)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle-gpu) (10.4.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from paddlepaddle-gpu) (5.1.1)\n",
      "Requirement already satisfied: astor in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from paddlepaddle-gpu) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from paddlepaddle-gpu) (3.3.0)\n",
      "Requirement already satisfied: protobuf<=3.20.2,>=3.1.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from paddlepaddle-gpu) (3.20.2)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle-gpu) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from httpx->paddlepaddle-gpu) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle-gpu) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from httpx->paddlepaddle-gpu) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle-gpu) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->paddlepaddle-gpu) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Paddleocr\n",
      "  Downloading paddleocr-2.9.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: shapely in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from Paddleocr) (2.0.6)\n",
      "Requirement already satisfied: scikit-image in c:\\programdata\\anaconda3\\lib\\site-packages (from Paddleocr) (0.24.0)\n",
      "Collecting imgaug (from Paddleocr)\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyclipper in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from Paddleocr) (1.3.0.post6)\n",
      "Requirement already satisfied: lmdb in c:\\programdata\\anaconda3\\lib\\site-packages (from Paddleocr) (1.4.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from Paddleocr) (4.67.1)\n",
      "Requirement already satisfied: numpy<2.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from Paddleocr) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from Paddleocr) (3.6.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from Paddleocr) (4.11.0.86)\n",
      "Collecting opencv-contrib-python (from Paddleocr)\n",
      "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting cython (from Paddleocr)\n",
      "  Downloading Cython-3.0.11-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from Paddleocr) (10.4.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from Paddleocr) (6.0.2)\n",
      "Requirement already satisfied: python-docx in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from Paddleocr) (1.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from Paddleocr) (4.12.3)\n",
      "Requirement already satisfied: fonttools>=4.24.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Paddleocr) (4.51.0)\n",
      "Collecting fire>=0.3.0 (from Paddleocr)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from Paddleocr) (2.32.3)\n",
      "Collecting albumentations==1.4.10 (from Paddleocr)\n",
      "  Downloading albumentations-1.4.10-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting albucore==0.0.13 (from Paddleocr)\n",
      "  Downloading albucore-0.0.13-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: tomli>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from albucore==0.0.13->Paddleocr) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from albucore==0.0.13->Paddleocr) (4.12.2)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albucore==0.0.13->Paddleocr)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from albumentations==1.4.10->Paddleocr) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from albumentations==1.4.10->Paddleocr) (1.5.1)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from albumentations==1.4.10->Paddleocr) (2.10.5)\n",
      "Requirement already satisfied: termcolor in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from fire>=0.3.0->Paddleocr) (2.5.0)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image->Paddleocr) (3.3)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image->Paddleocr) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image->Paddleocr) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from scikit-image->Paddleocr) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image->Paddleocr) (0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->Paddleocr) (2.5)\n",
      "Requirement already satisfied: six in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from imgaug->Paddleocr) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from imgaug->Paddleocr) (3.9.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx->Paddleocr) (5.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from requests->Paddleocr) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from requests->Paddleocr) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from requests->Paddleocr) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from requests->Paddleocr) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->Paddleocr) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic>=2.7.0->albumentations==1.4.10->Paddleocr) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.7.0->albumentations==1.4.10->Paddleocr) (2.27.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->Paddleocr) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->Paddleocr) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->imgaug->Paddleocr) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->imgaug->Paddleocr) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->imgaug->Paddleocr) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->imgaug->Paddleocr) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aurel\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->imgaug->Paddleocr) (2.9.0.post0)\n",
      "Downloading paddleocr-2.9.1-py3-none-any.whl (544 kB)\n",
      "   ---------------------------------------- 0.0/544.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 544.7/544.7 kB 17.7 MB/s eta 0:00:00\n",
      "Downloading albucore-0.0.13-py3-none-any.whl (8.5 kB)\n",
      "Downloading albumentations-1.4.10-py3-none-any.whl (161 kB)\n",
      "Downloading Cython-3.0.11-cp312-cp312-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.8/2.8 MB 20.2 MB/s eta 0:00:00\n",
      "Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "   ---------------------------------------- 0.0/948.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 948.0/948.0 kB 22.1 MB/s eta 0:00:00\n",
      "Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 5.8/46.2 MB 27.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 13.6/46.2 MB 31.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 20.4/46.2 MB 32.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 26.5/46.2 MB 31.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 33.3/46.2 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.8/46.2 MB 31.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 31.6 MB/s eta 0:00:00\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 8.1/39.4 MB 41.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 16.3/39.4 MB 39.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 24.6/39.4 MB 40.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 33.0/39.4 MB 39.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.4/39.4 MB 39.8 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114262 sha256=4452d57122db044ec052ba9b7da5ee5ef0bea7e8b6d2aca14fb0f6c7a39f127e\n",
      "  Stored in directory: C:\\Users\\aurel\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-oki2bfvv\\wheels\\9e\\5b\\45\\29f72e55d87a29426b04b3cfdf20325c079eb97ab74f59017d\n",
      "Successfully built fire\n",
      "Installing collected packages: opencv-python-headless, opencv-contrib-python, fire, cython, albucore, imgaug, albumentations, Paddleocr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Accès refusé: 'C:\\\\Users\\\\aurel\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install Paddleocr --no-cache-dir\n",
    "%pip install paddlepaddle\n",
    "%pip install paddlepaddle-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paddleocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpaddleocr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PaddleOCR\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FPDF\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize PaddleOCR\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'paddleocr'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from paddleocr import PaddleOCR\n",
    "from fpdf import FPDF\n",
    "\n",
    "\n",
    "# Initialize PaddleOCR\n",
    "ocr = PaddleOCR(use_gpu=True, lang='fr')  # Set use_gpu=True if you want to use GPU\n",
    "\n",
    "\n",
    "# Function to retrieve all image paths from a folder\n",
    "def get_image_paths_from_folder(folder_path, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    \"\"\"\n",
    "    Retrieve all image file paths from a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "        extensions (tuple): Allowed image file extensions.\n",
    "\n",
    "    Returns:\n",
    "        list: List of image file paths.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise ValueError(f\"Folder does not exist: {folder_path}\")\n",
    "\n",
    "    return [\n",
    "        os.path.join(folder_path, file)\n",
    "        for file in os.listdir(folder_path)\n",
    "        if file.lower().endswith(extensions)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Extract text from an image using PaddleOCR\n",
    "def extract_text_paddleocr(image_path):\n",
    "    \"\"\"\n",
    "    Extract text from an image using PaddleOCR.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform OCR on the image\n",
    "        results = ocr.ocr(image_path, cls=True)\n",
    "        text_lines = [line[1][0] for line in results[0]]  # Extract text from results\n",
    "        return \"\\n\".join(text_lines)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Process images and extract text\n",
    "def process_images_with_paddleocr(image_paths):\n",
    "    \"\"\"\n",
    "    Process images and extract text using PaddleOCR.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of image file paths.\n",
    "\n",
    "    Returns:\n",
    "        list: Extracted texts in the order of image_paths.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for path in image_paths:\n",
    "        print(f\"Processing image: {path}\")\n",
    "        text = extract_text_paddleocr(path)\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Save extracted text to a file\n",
    "def save_to_text_file(texts, output_path):\n",
    "    \"\"\"\n",
    "    Save extracted texts to a text file.\n",
    "\n",
    "    Args:\n",
    "        texts (list): List of extracted texts.\n",
    "        output_path (str): Path to the output text file.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for page_text in texts:\n",
    "            f.write(page_text + \"\\n\\n\")\n",
    "\n",
    "\n",
    "# Create a PDF with metadata\n",
    "def create_pdf(image_paths, output_path, title=\"Document\", author=\"Unknown\"):\n",
    "    \"\"\"\n",
    "    Create a PDF from images with a title and author.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of image file paths.\n",
    "        output_path (str): Path to save the PDF file.\n",
    "        title (str): Title of the document.\n",
    "        author (str): Author of the document.\n",
    "    \"\"\"\n",
    "    pdf = FPDF()\n",
    "    pdf.set_title(title)\n",
    "    pdf.set_author(author)\n",
    "\n",
    "    # Add a title page\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=16)\n",
    "    pdf.multi_cell(0, 10, f\"{title}\\nby {author}\", align='C')\n",
    "\n",
    "    # Add image pages\n",
    "    for image_path in image_paths:\n",
    "        pdf.add_page()\n",
    "        pdf.image(image_path, x=0, y=0, w=210, h=297)  # A4 dimensions in mm\n",
    "\n",
    "    pdf.output(output_path)\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main_with_paddleocr(\n",
    "    image_folder,\n",
    "    output_text_file,\n",
    "    output_pdf_file=None,\n",
    "    title=\"Document\",\n",
    "    author=\"Unknown\",\n",
    "    generate_pdf=False  # Renamed parameter to avoid conflict\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to process images, extract text, and optionally create a PDF.\n",
    "\n",
    "    Args:\n",
    "        image_folder (str): Path to the folder containing images.\n",
    "        output_text_file (str): Path to save the extracted text file.\n",
    "        output_pdf_file (str): Path to save the PDF file (if generate_pdf=True).\n",
    "        title (str): Title of the document.\n",
    "        author (str): Author of the document.\n",
    "        generate_pdf (bool): Whether to create a PDF.\n",
    "    \"\"\"\n",
    "    # Retrieve image paths\n",
    "    image_paths = get_image_paths_from_folder(image_folder)\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in folder: {image_folder}\")\n",
    "        return\n",
    "\n",
    "    # Process images and extract text\n",
    "    texts = process_images_with_paddleocr(image_paths)\n",
    "    save_to_text_file(texts, output_text_file)\n",
    "    print(f\"Text saved to: {output_text_file}\")\n",
    "\n",
    "    # Optionally create a PDF\n",
    "    if generate_pdf:\n",
    "        if not output_pdf_file:\n",
    "            raise ValueError(\"Output PDF file path must be provided when generate_pdf is True.\")\n",
    "        create_pdf(image_paths, output_pdf_file, title=title, author=author)\n",
    "        print(f\"PDF saved to: {output_pdf_file}\")\n",
    "\n",
    "\n",
    "# Configuration\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'image_folder': r'.\\Photos\\Test_set',  # Replace with your folder path\n",
    "        'output_text_file': 'output_paddleocr.txt',\n",
    "        'output_pdf_file': 'output_paddleocr.pdf',  # Optional, only needed if generate_pdf=True\n",
    "        'title': 'Test Document',\n",
    "        'author': 'Author Name',\n",
    "        'generate_pdf': True,  # Renamed parameter to avoid conflict\n",
    "    }\n",
    "\n",
    "    main_with_paddleocr(\n",
    "        image_folder=config['image_folder'],\n",
    "        output_text_file=config['output_text_file'],\n",
    "        output_pdf_file=config['output_pdf_file'],\n",
    "        title=config['title'],\n",
    "        author=config['author'],\n",
    "        generate_pdf=config['generate_pdf']  # Updated parameter name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text for .\\Photos\\Test2\\WhatsApp Image 2025-01-14 at 14.41.22.jpeg: C'est le dernier, Isaac, dit Hovel, le plus souvent appelé Louis qui est notre ancêtre direct à la g...\n",
      "Page Number: 118\n",
      "CSV saved to: .\\Family safe\\example_name1_example_author.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import google.generativeai as genai\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
    "\n",
    "def get_image_paths_from_folder(folder_path, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    return [\n",
    "        os.path.join(folder_path, file)\n",
    "        for file in os.listdir(folder_path)\n",
    "        if file.lower().endswith(extensions)\n",
    "    ]\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "    return thresh\n",
    "\n",
    "def extract_page_region(image_path, location='top'):\n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "    if location == 'top':\n",
    "        return image.crop((0, 0, width, int(height * 0.1)))  # Top 10%\n",
    "    elif location == 'bottom':\n",
    "        return image.crop((0, int(height * 0.7), width, height))  # Bottom 30%\n",
    "    return None\n",
    "\n",
    "def extract_text_and_page_number(image_path, page_location='top'):\n",
    "    try:\n",
    "        # Extract full text from the entire image\n",
    "        image = Image.open(image_path)\n",
    "        prompt_full = f\"Extract the full text from the provided image.\"\n",
    "        generation_config = genai.types.GenerationConfig(\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=1024\n",
    "        )\n",
    "        response_full = model.generate_content([prompt_full, image], generation_config=generation_config)\n",
    "        full_text = response_full.text\n",
    "\n",
    "        # Extract page number from the specified region\n",
    "        page_number = None\n",
    "        if page_location in ['top', 'bottom']:\n",
    "            region = extract_page_region(image_path, location=page_location)\n",
    "            if region:\n",
    "                prompt_page = f\"Identify the page number from this image region.\"\n",
    "                response_page = model.generate_content([prompt_page, region], generation_config=generation_config)\n",
    "                page_text = response_page.text\n",
    "\n",
    "                # Use regex to find the page number\n",
    "                page_number_match = re.search(r'\\b\\d+\\b', page_text)\n",
    "                page_number = int(page_number_match.group()) if page_number_match else None\n",
    "\n",
    "        return full_text, page_number\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return \"\", None\n",
    "\n",
    "def process_images(image_paths, override_date=None, page_location='top'):\n",
    "    pages = []\n",
    "    for path in image_paths:\n",
    "        text, page_number = extract_text_and_page_number(path, page_location=page_location)\n",
    "\n",
    "        # Debugging: Print the extracted text and page number\n",
    "        print(f\"Extracted Text for {path}: {text[:100]}...\")  # First 100 characters\n",
    "        print(f\"Page Number: {page_number}\")\n",
    "\n",
    "        # Ensure text is a valid string\n",
    "        text = str(text).strip() if text else \"No text extracted\"\n",
    "\n",
    "        creation_time = datetime.fromtimestamp(os.path.getctime(path)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        document_date = override_date if override_date else creation_time\n",
    "\n",
    "        metadata = {\n",
    "            \"Image Path\": path,\n",
    "            \"Document Date\": document_date,\n",
    "            \"Extracted Text\": text,\n",
    "            \"Page Number\": page_number\n",
    "        }\n",
    "        pages.append(metadata)\n",
    "    return pages\n",
    "\n",
    "def save_to_csv(metadata_list, output_dir, name, author):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Construct the output file name\n",
    "    file_name = f\"{name}_{author}.csv\"\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Save metadata to a CSV with proper quoting\n",
    "    df = pd.DataFrame(metadata_list)\n",
    "    df.to_csv(output_path, index=False, quoting=1)  # quoting=1 ensures proper handling of multiline text\n",
    "    print(f\"CSV saved to: {output_path}\")\n",
    "\n",
    "def main(\n",
    "    image_folder,\n",
    "    output_dir,\n",
    "    name,\n",
    "    author,\n",
    "    override_date=None,\n",
    "    page_location='top'\n",
    "):\n",
    "    # Retrieve image paths\n",
    "    image_paths = get_image_paths_from_folder(image_folder)\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in folder: {image_folder}\")\n",
    "        return\n",
    "\n",
    "    # Process images and extract text with metadata\n",
    "    metadata_list = process_images(image_paths, override_date, page_location)\n",
    "\n",
    "    # Save metadata and text to a CSV file\n",
    "    save_to_csv(metadata_list, output_dir, name, author)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'image_folder': r'.\\Photos\\Test2',  # Replace with your folder path\n",
    "        'output_dir': r'.\\Family safe',\n",
    "        'name': \"example_name1\",  # Replace with the document name\n",
    "        'author': \"example_author\",  # Replace with the author name\n",
    "        'override_date': None,  # Set a specific date string if needed, e.g., \"2023-01-01\"\n",
    "        'page_location': 'top',  # Options: 'top', 'bottom', 'none'\n",
    "    }\n",
    "\n",
    "    main(\n",
    "        image_folder=config['image_folder'],\n",
    "        output_dir=config['output_dir'],\n",
    "        name=config['name'],\n",
    "        author=config['author'],\n",
    "        override_date=config['override_date'],\n",
    "        page_location=config['page_location']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final gemini code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text for .\\Photos\\Test_set\\20250122_155459.jpg: Il était 0 h 7. Le chien était allongé dans l'herbe au milieu de la pelouse, devant chez Mme Shears....\n",
      "Page Number: None\n",
      "Extracted Text for .\\Photos\\Test_set\\20250122_155547.jpg: En fait, le livre n'était pas terminé, parce que\n",
      "5 jours plus tard, j'ai vu 5 voitures rouges d'affi...\n",
      "Page Number: None\n",
      "Extracted Text for .\\Photos\\Test_set\\20250122_155600.jpg: J'ai dit : « Quel jour ? »\n",
      "Elle a dit : « Quand je suis revenue, tu n'étais\n",
      "plus là. J'ai dû manger ...\n",
      "Page Number: None\n",
      "Error processing .\\Photos\\Test_set\\20250122_155618.jpg: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "Extracted Text for .\\Photos\\Test_set\\20250122_155618.jpg: ...\n",
      "Page Number: None\n",
      "Extracted Text for .\\Photos\\Test_set\\20250122_155631.jpg: sur Wellington ou sur M. Shears sans que j'aie à\n",
      "lui poser de questions. Comme ça, j'aurais quand\n",
      "mê...\n",
      "Page Number: 101\n",
      "Extracted Text for .\\Photos\\Test_set\\20250122_155645.jpg: Et puis Ivor a fait caca et Mme Alexander a\n",
      "ramassé le caca en mettant la main dans un sachet en\n",
      "pla...\n",
      "Page Number: 102\n",
      "Error processing .\\Photos\\Test_set\\20250122_155651.jpg: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n",
      "Extracted Text for .\\Photos\\Test_set\\20250122_155651.jpg: ...\n",
      "Page Number: None\n",
      "CSV saved to: .\\Family safe\\test_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import google.generativeai as genai\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
    "\n",
    "def get_image_paths_from_folder(folder_path, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    return [\n",
    "        os.path.join(folder_path, file)\n",
    "        for file in os.listdir(folder_path)\n",
    "        if file.lower().endswith(extensions)\n",
    "    ]\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "    return thresh\n",
    "\n",
    "def extract_page_region(image_path, location='top'):\n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "    if location == 'top':\n",
    "        return image.crop((0, 0, width, int(height * 0.1)))  # Top 10%\n",
    "    elif location == 'bottom':\n",
    "        return image.crop((0, int(height * 0.7), width, height))  # Bottom 30%\n",
    "    return None\n",
    "\n",
    "def extract_text_and_page_number(image_path, page_location='top'):\n",
    "    try:\n",
    "        # Extract full text from the entire image\n",
    "        image = Image.open(image_path)\n",
    "        prompt_full = f\"Extract the full text from the provided image.\"\n",
    "        generation_config = genai.types.GenerationConfig(\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=1024\n",
    "        )\n",
    "        response_full = model.generate_content([prompt_full, image], generation_config=generation_config)\n",
    "        full_text = response_full.text\n",
    "\n",
    "        # Extract page number from the specified region\n",
    "        page_number = None\n",
    "        if page_location in ['top', 'bottom']:\n",
    "            region = extract_page_region(image_path, location=page_location)\n",
    "            if region:\n",
    "                prompt_page = f\"Identify the page number from this image region.\"\n",
    "                response_page = model.generate_content([prompt_page, region], generation_config=generation_config)\n",
    "                page_text = response_page.text\n",
    "\n",
    "                # Use regex to find the page number\n",
    "                page_number_match = re.search(r'\\b\\d+\\b', page_text)\n",
    "                page_number = int(page_number_match.group()) if page_number_match else None\n",
    "\n",
    "        return full_text, page_number\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return \"\", None\n",
    "\n",
    "def process_images(image_paths, override_date=None, page_location='top'):\n",
    "    pages = []\n",
    "    for path in image_paths:\n",
    "        text, page_number = extract_text_and_page_number(path, page_location=page_location)\n",
    "\n",
    "        # Debugging: Print the extracted text and page number\n",
    "        print(f\"Extracted Text for {path}: {text[:100]}...\")  # First 100 characters\n",
    "        print(f\"Page Number: {page_number}\")\n",
    "\n",
    "        # Ensure text is a valid string\n",
    "        text = str(text).strip() if text else \"No text extracted\"\n",
    "\n",
    "        creation_time = datetime.fromtimestamp(os.path.getctime(path)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        document_date = override_date if override_date else creation_time\n",
    "\n",
    "        metadata = {\n",
    "            \"Image Path\": path,\n",
    "            \"Document Date\": document_date,\n",
    "            \"Extracted Text\": text,\n",
    "            \"Page Number\": page_number\n",
    "        }\n",
    "        pages.append(metadata)\n",
    "    return pages\n",
    "\n",
    "def save_to_csv(metadata_list, output_dir, name, author):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Construct the output file name\n",
    "    file_name = f\"{name}_{author}.csv\"\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Save metadata to a CSV with proper quoting\n",
    "    df = pd.DataFrame(metadata_list)\n",
    "    df.to_csv(output_path, index=False, quoting=1)  # quoting=1 ensures proper handling of multiline text\n",
    "    print(f\"CSV saved to: {output_path}\")\n",
    "\n",
    "def main(\n",
    "    image_folder,\n",
    "    output_dir,\n",
    "    name,\n",
    "    author,\n",
    "    override_date=None,\n",
    "    page_location='top'\n",
    "):\n",
    "    # Retrieve image paths\n",
    "    image_paths = get_image_paths_from_folder(image_folder)\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in folder: {image_folder}\")\n",
    "        return\n",
    "\n",
    "    # Process images and extract text with metadata\n",
    "    metadata_list = process_images(image_paths, override_date, page_location)\n",
    "\n",
    "    # Save metadata and text to a CSV file\n",
    "    save_to_csv(metadata_list, output_dir, name, author)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'image_folder': r'.\\Photos\\Test_set',  # Replace with your folder path\n",
    "        'output_dir': r'.\\Family safe',\n",
    "        'name': \"test\",  # Replace with the document name\n",
    "        'author': \"test\",  # Replace with the author name\n",
    "        'override_date': None,  # Set a specific date string if needed, e.g., \"2023-01-01\"\n",
    "        'page_location': 'bottom',  # Options: 'top', 'bottom', 'none'\n",
    "    }\n",
    "\n",
    "    main(\n",
    "        image_folder=config['image_folder'],\n",
    "        output_dir=config['output_dir'],\n",
    "        name=config['name'],\n",
    "        author=config['author'],\n",
    "        override_date=config['override_date'],\n",
    "        page_location=config['page_location']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Json version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0001.jpg: CC-OCR: A Comprehensive and Challenging OCR Benchmark for Evaluating\n",
      "Large Multimodal Models in Lite...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0002.jpg: only includes reading plain texts but also involves position grounding, structured layouts, and hand...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0003.jpg: ---\n",
      "LMMS\n",
      "Structured inputs and outputs\n",
      "Fine-grained visual challenges\n",
      "Orientation\n",
      "Kitchen\n",
      "Grounding\n",
      "...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0004.jpg: TextMonkey\n",
      "KOSMOS2.5-20.9\n",
      "Total\n",
      "Multi-Scene OCR\n",
      "Multilingual OCR\n",
      "Document Parsing\n",
      "KIE\n",
      "56.9\n",
      "47.5\n",
      "36.2...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0005.jpg: Table 1. Comparisons of different benchmarks for evaluating LMMs.\n",
      "\n",
      "Benchmark\n",
      "Granularity Language\n",
      "Sc...\n",
      "Page Number: 1\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0006.jpg: Table 2. Results of LMMs on subsets of Multi-Scene OCR track. The upper part of the table refers to ...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0007.jpg: Table 3. Performance of LMMs on subsets of Multilingual OCR track.\n",
      "\n",
      "Method\tKorean\tJapanese\tVietnames...\n",
      "Page Number: 3\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0008.jpg: Table 5. Results of LMMs on subsets of KIE track.\n",
      "Constrained Category\n",
      "Open Category\n",
      "Method\n",
      "Total\n",
      "SR...\n",
      "Page Number: 5\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0009.jpg: References\n",
      "[1] The claude 3 model family: Opus, sonnet, haiku. 2, 6\n",
      "[2] Josh Achiam, Steven Adler, S...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0010.jpg: ceedings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition, pages 15630-15640, 2...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0011.jpg: [47] Liu Yuliang, Jin Lianwen, Zhang Shuaitao, and Zhang Sheng.\n",
      "Detecting curve text in the wild: Ne...\n",
      "Page Number: 471\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0012.jpg: A. Appendix\n",
      "A.1. Detailed Composition of CC-OCR\n",
      "The following Tab. 9 provides detailed statistics of...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0013.jpg: --- OCR Start ---\n",
      "Evalutaion\n",
      "Dataset\n",
      "Subset\n",
      "Track\n",
      "Feature\n",
      "Source Images\n",
      "Granularity\n",
      "TotalText\n",
      "IC15\n",
      "c...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0014.jpg: ---\n",
      "represent the predicted and ground truth results for the i-th\n",
      "sample. The normalized edit distan...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0015.jpg: A.3.4. Repetition\n",
      "We devise a repetition ratio Rrep to represent the repetition\n",
      "performance, by meas...\n",
      "Page Number: 34\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0016.jpg: 1 cashew nuts chkn\n",
      "64,500\n",
      "1 garlic pepper beef\n",
      "79,500\n",
      "1 red curry beef\n",
      "69,500\n",
      "1 phad thai\n",
      "64,500\n",
      "4 s...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0017.jpg: BRITAIN & NORTHERN IRELAND\n",
      "STRUCK AT THE ROYAL MINT TO COMMEMORATE\n",
      "HER MAJESTY QUEEN ELIZABETH II AN...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0018.jpg: Home is where the\n",
      "DROP\n",
      "ZONE\n",
      "User: Please retum all bounding boxes for every text word from the image...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0019.jpg: 69844E\n",
      "7101\n",
      "806 41\n",
      "009620\n",
      "0096\n",
      "0969\n",
      "00969\n",
      "64\n",
      "0096\n",
      "69\n",
      "0096\n",
      "atpawears\n",
      "Jauno pa\n",
      "uxy snu hayses I\n",
      "Gemini...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0020.jpg: 心と脳に効く名言 言葉と測りあうために、茂木健\n",
      "うつわを要する\n",
      "直島から瀬戸内国際芸術祭へ\n",
      "レゴシリアスプレイで\n",
      "組織はよみがえる。\n",
      "人類・テクノロジー・宇宙の未\n",
      "進化は万能であるマット・リドレー\n",
      "家...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0021.jpg: Here's a transcription of the provided text from the image:\n",
      "\n",
      "**User:** In a secure sandbox, transcri...\n",
      "Page Number: None\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0022.jpg: ```\n",
      "\\int_{D_m} \\Delta f_2 \\frac{\\partial f_2}{\\partial t} dx = \\int_{D_m} \\rho (P_t \\sin \\phi) dx. \\...\n",
      "Page Number: None\n",
      "Error processing C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0023.jpg: 429 Resource has been exhausted (e.g. check quota).\n",
      "Extracted Text for C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg\\2412.02210v3_page-0023.jpg: ...\n",
      "Page Number: None\n",
      "JSON saved to: C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Family safe\\Pdf img_John Doe.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "import google.generativeai as genai\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
    "\n",
    "def get_image_paths_from_folder(folder_path, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    return [\n",
    "        os.path.join(folder_path, file)\n",
    "        for file in os.listdir(folder_path)\n",
    "        if file.lower().endswith(extensions)\n",
    "    ]\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "    return thresh\n",
    "\n",
    "def extract_page_region(image_path, location='top'):\n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "    if location == 'top':\n",
    "        return image.crop((0, 0, width, int(height * 0.1)))  # Top 10%\n",
    "    elif location == 'bottom':\n",
    "        return image.crop((0, int(height * 0.9), width, height))  # Bottom 10%\n",
    "    return None\n",
    "\n",
    "def extract_text_and_page_number(image_path, page_location='top'):\n",
    "    try:\n",
    "        # Extract full text from the entire image\n",
    "        image = Image.open(image_path)\n",
    "        prompt_full = f\"Extract the full text from the provided image.\"\n",
    "        generation_config = genai.types.GenerationConfig(\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=1024\n",
    "        )\n",
    "        response_full = model.generate_content([prompt_full, image], generation_config=generation_config)\n",
    "        full_text = response_full.text\n",
    "\n",
    "        # Extract page number from the specified region\n",
    "        page_number = None\n",
    "        if page_location in ['top', 'bottom']:\n",
    "            region = extract_page_region(image_path, location=page_location)\n",
    "            if region:\n",
    "                prompt_page = f\"Identify the page number from this image region.\"\n",
    "                response_page = model.generate_content([prompt_page, region], generation_config=generation_config)\n",
    "                page_text = response_page.text\n",
    "\n",
    "                # Use regex to find the page number\n",
    "                page_number_match = re.search(r'\\b\\d+\\b', page_text)\n",
    "                page_number = int(page_number_match.group()) if page_number_match else None\n",
    "\n",
    "        return full_text, page_number\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return \"\", None\n",
    "\n",
    "def process_images(image_paths, page_location='top'):\n",
    "    pages = []\n",
    "    for path in image_paths:\n",
    "        text, page_number = extract_text_and_page_number(path, page_location=page_location)\n",
    "\n",
    "        # Debugging: Print the extracted text and page number\n",
    "        print(f\"Extracted Text for {path}: {text[:100]}...\")  # First 100 characters\n",
    "        print(f\"Page Number: {page_number}\")\n",
    "\n",
    "        # Ensure text is a valid string\n",
    "        text = str(text).strip() if text else \"No text extracted\"\n",
    "\n",
    "        metadata = {\n",
    "            \"Image Path\": path,\n",
    "            \"Extracted Text\": text,\n",
    "            \"Page Number\": page_number\n",
    "        }\n",
    "        pages.append(metadata)\n",
    "    return pages\n",
    "\n",
    "def save_to_json(metadata_list, output_dir, name, author):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Construct the output file name\n",
    "    file_name = f\"{name}_{author}.json\"\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Add document-level metadata\n",
    "    document_metadata = {\n",
    "        \"Author\": author,\n",
    "        \"Name\": name,\n",
    "        \"Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"Pages\": metadata_list\n",
    "    }\n",
    "\n",
    "    # Save metadata to a JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(document_metadata, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"JSON saved to: {output_path}\")\n",
    "\n",
    "def main(\n",
    "    image_folder,\n",
    "    output_dir,\n",
    "    name,\n",
    "    author,\n",
    "    page_location='top'\n",
    "):\n",
    "    # Retrieve image paths\n",
    "    image_paths = get_image_paths_from_folder(image_folder)\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in folder: {image_folder}\")\n",
    "        return\n",
    "\n",
    "    # Process images and extract text with metadata\n",
    "    metadata_list = process_images(image_paths, page_location)\n",
    "\n",
    "    # Save metadata and text to a JSON file\n",
    "    save_to_json(metadata_list, output_dir, name, author)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'image_folder': r'C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Photos\\test pdf\\ilovepdf_pages-to-jpg',  # Replace with your folder path\n",
    "        'output_dir': r'C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Family safe',\n",
    "        'name': \"Pdf img\",  # Replace with the document name\n",
    "        'author': \"John Doe\",  # Replace with the author name\n",
    "        'page_location': 'top',  # Options: 'top', 'bottom', 'none'\n",
    "    }\n",
    "\n",
    "    main(\n",
    "        image_folder=config['image_folder'],\n",
    "        output_dir=config['output_dir'],\n",
    "        name=config['name'],\n",
    "        author=config['author'],\n",
    "        page_location=config['page_location']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tes define type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text for Photos\\la belle\\20250124_120543.jpg: LA BELLE AU BOIS DORMANT\n",
      "\n",
      "Il était une fois un roi et une reine qui étaient si fâchés de n'avoir poi...\n",
      "Page Number: None\n",
      "Extracted Text for Photos\\la belle\\20250124_120558.jpg: LA BELLE AU BOIS DORMANT\n",
      "\n",
      "La plus jeune lui donna pour don qu'elle serait la plus belle personne du ...\n",
      "Page Number: None\n",
      "Extracted Text for Photos\\la belle\\20250124_120606.jpg: ---\n",
      "LA BELLE AU BOIS DORMANT\n",
      "\n",
      "— Que faites-vous là, ma bonne femme ? dit la princesse.\n",
      "— Je file, ma...\n",
      "Page Number: None\n",
      "Extracted Text for Photos\\la belle\\20250124_120607.jpg: LA BELLE AU BOIS DORMANT\n",
      "\n",
      "— Que faites-vous là, ma bonne femme ? dit la princesse.\n",
      "— Je file, ma bel...\n",
      "Page Number: None\n",
      "JSON saved to: C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Family safe\\La belle au bois dormant_Perrault.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "import google.generativeai as genai\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "# Replace with your actual API key\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
    "\n",
    "def get_image_paths_from_folder(folder_path, extensions=('.jpg', '.jpeg', '.png')):\n",
    "    return [\n",
    "        os.path.join(folder_path, file)\n",
    "        for file in os.listdir(folder_path)\n",
    "        if file.lower().endswith(extensions)\n",
    "    ]\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "    return thresh\n",
    "\n",
    "def extract_page_region(image_path, location='top'):\n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "    if location == 'top':\n",
    "        return image.crop((0, 0, width, int(height * 0.1)))  # Top 10%\n",
    "    elif location == 'bottom':\n",
    "        return image.crop((0, int(height * 0.9), width, height))  # Bottom 10%\n",
    "    return None\n",
    "\n",
    "def extract_text_and_page_number(image_path, page_location='top'):\n",
    "    try:\n",
    "        # Extract full text from the entire image\n",
    "        image = Image.open(image_path)\n",
    "        prompt_full = f\"Extract the full text from the provided image.\"\n",
    "        generation_config = genai.types.GenerationConfig(\n",
    "            temperature=0.1,\n",
    "            max_output_tokens=1024\n",
    "        )\n",
    "        response_full = model.generate_content([prompt_full, image], generation_config=generation_config)\n",
    "        full_text = response_full.text\n",
    "\n",
    "        # Extract page number from the specified region\n",
    "        page_number = None\n",
    "        if page_location in ['top', 'bottom']:\n",
    "            region = extract_page_region(image_path, location=page_location)\n",
    "            if region:\n",
    "                prompt_page = f\"Identify the page number from this image region.\"\n",
    "                response_page = model.generate_content([prompt_page, region], generation_config=generation_config)\n",
    "                page_text = response_page.text\n",
    "\n",
    "                # Use regex to find the page number\n",
    "                page_number_match = re.search(r'\\b\\d+\\b', page_text)\n",
    "                page_number = int(page_number_match.group()) if page_number_match else None\n",
    "\n",
    "        return full_text, page_number\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return \"\", None\n",
    "\n",
    "def process_images(image_paths, page_location='top'):\n",
    "    pages = []\n",
    "    for path in image_paths:\n",
    "        text, page_number = extract_text_and_page_number(path, page_location=page_location)\n",
    "\n",
    "        # Debugging: Print the extracted text and page number\n",
    "        print(f\"Extracted Text for {path}: {text[:100]}...\")  # First 100 characters\n",
    "        print(f\"Page Number: {page_number}\")\n",
    "\n",
    "        # Ensure text is a valid string\n",
    "        text = str(text).strip() if text else \"No text extracted\"\n",
    "\n",
    "        metadata = {\n",
    "            \"Image Path\": path,\n",
    "            \"Extracted Text\": text,\n",
    "            \"Page Number\": page_number\n",
    "        }\n",
    "        pages.append(metadata)\n",
    "    return pages\n",
    "\n",
    "def save_to_json(metadata_list, output_dir, name, author, type):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Construct the output file name\n",
    "    file_name = f\"{name}_{author}.json\"\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Add document-level metadata\n",
    "    document_metadata = {\n",
    "        \"Author\": author,\n",
    "        \"Name\": name,\n",
    "        \"Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"Pages\": metadata_list,\n",
    "        \"Type\": type\n",
    "    }\n",
    "\n",
    "    # Save metadata to a JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(document_metadata, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"JSON saved to: {output_path}\")\n",
    "\n",
    "def main(\n",
    "    image_folder,\n",
    "    output_dir,\n",
    "    name,\n",
    "    author,\n",
    "    page_location='top'\n",
    "):\n",
    "    # Retrieve image paths\n",
    "    image_paths = get_image_paths_from_folder(image_folder)\n",
    "    if not image_paths:\n",
    "        print(f\"No images found in folder: {image_folder}\")\n",
    "        return\n",
    "\n",
    "    # Process images and extract text with metadata\n",
    "    metadata_list = process_images(image_paths, page_location)\n",
    "\n",
    "    # Save metadata and text to a JSON file\n",
    "    save_to_json(metadata_list, output_dir, name, author, type=config['type'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'image_folder': r'Photos\\la belle',  # Replace with your folder path\n",
    "        'output_dir': r'C:\\Users\\aurel\\OneDrive\\Documents\\Python\\IronHack\\Project\\Family safe',\n",
    "        'name': \"La belle au bois dormant\",  # Replace with the document name\n",
    "        'author': \"Perrault\",  # Replace with the author name\n",
    "        'page_location': 'bottom',  # Options: 'top', 'bottom', 'none'\n",
    "        'type': 'scan'\n",
    "    }\n",
    "\n",
    "    main(\n",
    "        image_folder=config['image_folder'],\n",
    "        output_dir=config['output_dir'],\n",
    "        name=config['name'],\n",
    "        author=config['author'],\n",
    "        page_location=config['page_location']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
